{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple LLM-Powered Agent in Python\n",
    "\n",
    "Welcome to this tutorial on building a simple agent in Python! In this notebook, we will explore the basics of Large Language Models (LLMs) and prompts. The focus will be on understanding LLMs, user prompts, system prompts, and how to communicate with LLMs effectively. \n",
    "\n",
    "By the end of this tutorial, you'll understand how to configure a simple LLM, craft effective prompts, and make your first call to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Prompts\n",
    "\n",
    "### What is a Large Language Model (LLM)?\n",
    "\n",
    "A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human-like text. These models are trained on vast amounts of data and can be used for tasks like answering questions, summarizing information, generating content, and more.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to get a desired output. It typically consists of two parts:\n",
    "\n",
    "- **System Prompt:** This defines the context, role, or instructions for the model.\n",
    "- **User Prompt:** This is the specific request or question that the user wants the LLM to answer.\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "- **System Prompt:** Defines the role or behavior of the LLM.\n",
    "- **User Prompt:** The specific input provided by the user.\n",
    "- **Temperature:** Controls the randomness of the model's output. Higher values produce more creative responses, while lower values produce more deterministic ones.\n",
    "- **Stop Sequence:** Instructs the model where to stop generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "1. **Start:** The interaction begins with defining the task.\n",
    "2. **Crafting Prompts:** The user creates two types of prompts:\n",
    "   - **System Prompt:** Defines the role or behavior of the LLM (e.g., an assistant, a teacher, etc.).\n",
    "   - **User Prompt:** The user's actual question or request (e.g., \"What are the key features of LLMs?\").\n",
    "3. **Prepare Payload:** The prompts, along with any parameters (e.g., temperature, stop sequences), are packaged into a payload to be sent to the LLM.\n",
    "4. **LLM Interaction:** The payload is sent to the LLM API. This triggers the internal processes of the LLM.\n",
    "5. **LLM Internal Processing (Subgraph):** The internal processes include:\n",
    "   - **Tokenization:** The LLM splits the input text into smaller units (tokens) that it can process.\n",
    "   - **Inference/Computation:** The LLM uses its neural network to compute the output based on the tokens and context.\n",
    "   - **Detokenization:** The LLM converts the generated tokens back into human-readable text.\n",
    "   - **Post-Processing:** Any additional processing (e.g., truncating or adjusting based on temperature or stop sequences).\n",
    "6. **Receive & Process Response:** Once the LLM finishes processing, the response is received and can be formatted or processed further if needed.\n",
    "7. **Output Result:** The final output is presented to the user, displaying the LLM’s response to the original prompt.\n",
    "8. **End:** The process concludes once the result has been delivered.\n",
    "\n",
    "![image](images/llm-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `requests` library to send our prompts to the LLM endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring the LLM\n",
    "\n",
    "Next, we’ll set up a configuration to define which model to use and other parameters like temperature and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:latest', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model(model=\"llama3.1:latest\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "llm_config = setup_llm_model()\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the configuration for the LLM, including the endpoint and parameters like temperature and stop sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crafting the Prompts\n",
    " \n",
    "In the context of Large Language Models (LLMs), **prompts** are crucial for guiding the model's behavior and obtaining useful outputs. They consist of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system and user prompts\n",
    "sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>\"\n",
    "\n",
    "user_request = \"What are the key features of Large Language Models?\"\n",
    "user_prompt = (\n",
    "    f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3.1:latest', 'prompt': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key features of Large Language Models?<|eot_id|>', 'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>', 'temperature': 0.0, 'stop': None, 'stream': False}\n"
     ]
    }
   ],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, config: dict):\n",
    "    return {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"stop\": config[\"stop\"],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=sys_prompt, config=llm_config\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload contains the system and user prompts along with the LLM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making a Call to the LLM\n",
    "\n",
    "Now that we have the prompts prepared, we’ll send the request to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:latest', 'created_at': '2024-09-06T17:51:59.977428Z', 'response': 'Large Language Models (LLMs) have several key features:\\n\\n1. **Scalability**: LLMs can process vast amounts of text data, making them capable of learning and generating human-like language.\\n2. **Self-supervised learning**: They learn from large datasets without explicit supervision, allowing them to identify patterns and relationships in the text.\\n3. **Sequence-to-Sequence architecture**: LLMs are typically based on a sequence-to-sequence model, which allows them to take input sequences (e.g., sentences) and output sequences of text.\\n4. **Multi-layer Transformer architecture**: Many modern LLMs use a multi-layer transformer architecture, which enables parallelization and efficient processing of sequential data.\\n5. **Transformer layers with self-attention**: These models utilize self-attention mechanisms, allowing the model to focus on specific parts of the input sequence when generating output.\\n6. **Pre-training and fine-tuning**: LLMs are often pre-trained on large datasets (e.g., Wikipedia) and then fine-tuned on smaller, task-specific datasets for more accurate performance.\\n7. **Long-range dependencies**: They can capture long-range dependencies between words or phrases in a sentence, enabling the model to better understand context and nuances of language.\\n8. **High computational efficiency**: LLMs are designed to be computationally efficient, allowing them to process large volumes of text data quickly.\\n9. **Flexibility**: These models can be applied to various natural language processing (NLP) tasks, such as language translation, sentiment analysis, or text classification.\\n\\nThese features contribute to the impressive capabilities and versatility of Large Language Models in understanding and generating human-like language.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 128000, 128006, 9125, 128007, 2675, 527, 264, 11190, 18328, 430, 5825, 64694, 323, 13687, 11503, 13, 128009, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 35353, 11688, 27972, 320, 4178, 22365, 8, 617, 3892, 1401, 4519, 1473, 16, 13, 3146, 3407, 278, 2968, 96618, 445, 11237, 82, 649, 1920, 13057, 15055, 315, 1495, 828, 11, 3339, 1124, 13171, 315, 6975, 323, 24038, 3823, 12970, 4221, 627, 17, 13, 3146, 12363, 59615, 79090, 6975, 96618, 2435, 4048, 505, 3544, 30525, 2085, 11720, 38217, 11, 10923, 1124, 311, 10765, 12912, 323, 12135, 304, 279, 1495, 627, 18, 13, 3146, 14405, 4791, 12, 14405, 18112, 96618, 445, 11237, 82, 527, 11383, 3196, 389, 264, 8668, 4791, 7962, 4474, 1646, 11, 902, 6276, 1124, 311, 1935, 1988, 24630, 320, 68, 1326, 2637, 23719, 8, 323, 2612, 24630, 315, 1495, 627, 19, 13, 3146, 20981, 48435, 63479, 18112, 96618, 9176, 6617, 445, 11237, 82, 1005, 264, 7447, 48435, 43678, 18112, 11, 902, 20682, 15638, 2065, 323, 11297, 8863, 315, 52100, 828, 627, 20, 13, 3146, 47458, 13931, 449, 659, 12, 54203, 96618, 4314, 4211, 29166, 659, 12, 54203, 24717, 11, 10923, 279, 1646, 311, 5357, 389, 3230, 5596, 315, 279, 1988, 8668, 994, 24038, 2612, 627, 21, 13, 3146, 4808, 86470, 323, 7060, 2442, 38302, 96618, 445, 11237, 82, 527, 3629, 864, 70024, 389, 3544, 30525, 320, 68, 1326, 2637, 27685, 8, 323, 1243, 7060, 2442, 49983, 389, 9333, 11, 3465, 19440, 30525, 369, 810, 13687, 5178, 627, 22, 13, 3146, 6720, 31608, 20113, 96618, 2435, 649, 12602, 1317, 31608, 20113, 1990, 4339, 477, 32847, 304, 264, 11914, 11, 28462, 279, 1646, 311, 2731, 3619, 2317, 323, 84889, 315, 4221, 627, 23, 13, 3146, 12243, 55580, 15374, 96618, 445, 11237, 82, 527, 6319, 311, 387, 3801, 30154, 11297, 11, 10923, 1124, 311, 1920, 3544, 27378, 315, 1495, 828, 6288, 627, 24, 13, 3146, 32771, 3225, 96618, 4314, 4211, 649, 387, 9435, 311, 5370, 5933, 4221, 8863, 320, 45, 12852, 8, 9256, 11, 1778, 439, 4221, 14807, 11, 27065, 6492, 11, 477, 1495, 24790, 382, 9673, 4519, 17210, 311, 279, 16358, 17357, 323, 60112, 315, 20902, 11688, 27972, 304, 8830, 323, 24038, 3823, 12970, 4221, 13], 'total_duration': 6728431875, 'load_duration': 30927375, 'prompt_eval_count': 47, 'prompt_eval_duration': 324631000, 'eval_count': 336, 'eval_duration': 6371877000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_request_to_llm(payload: dict, config: dict):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config[\"model_endpoint\"],\n",
    "            headers=config[\"headers\"],\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Send the request\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prompt to the LLM and handles any errors that might occur during the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the LLM Response\n",
    "\n",
    "Finally, let's process the response and display the relevant information in a user-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed response: Large Language Models (LLMs) have several key features:\n",
      "\n",
      "1. **Scalability**: LLMs can process vast amounts of text data, making them capable of learning and generating human-like language.\n",
      "2. **Self-supervised learning**: They learn from large datasets without explicit supervision, allowing them to identify patterns and relationships in the text.\n",
      "3. **Sequence-to-Sequence architecture**: LLMs are typically based on a sequence-to-sequence model, which allows them to take input sequences (e.g., sentences) and output sequences of text.\n",
      "4. **Multi-layer Transformer architecture**: Many modern LLMs use a multi-layer transformer architecture, which enables parallelization and efficient processing of sequential data.\n",
      "5. **Transformer layers with self-attention**: These models utilize self-attention mechanisms, allowing the model to focus on specific parts of the input sequence when generating output.\n",
      "6. **Pre-training and fine-tuning**: LLMs are often pre-trained on large datasets (e.g., Wikipedia) and then fine-tuned on smaller, task-specific datasets for more accurate performance.\n",
      "7. **Long-range dependencies**: They can capture long-range dependencies between words or phrases in a sentence, enabling the model to better understand context and nuances of language.\n",
      "8. **High computational efficiency**: LLMs are designed to be computationally efficient, allowing them to process large volumes of text data quickly.\n",
      "9. **Flexibility**: These models can be applied to various natural language processing (NLP) tasks, such as language translation, sentiment analysis, or text classification.\n",
      "\n",
      "These features contribute to the impressive capabilities and versatility of Large Language Models in understanding and generating human-like language.\n"
     ]
    }
   ],
   "source": [
    "def process_response(response: dict):\n",
    "    if \"error\" in response:\n",
    "        return f\"Error: {response['error']}\"\n",
    "    return response.get(\"response\", \"No response from the model\")\n",
    "\n",
    "# Process the LLM response\n",
    "processed_response = process_response(response)\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function ensures that we handle any errors gracefully and return the model's response in a readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've simplified the process of interacting with an LLM. We've covered:\n",
    "- **LLMs and Prompts:** Understanding system and user prompts.\n",
    "- **Model Configuration:** How to configure the LLM for use.\n",
    "- **Sending Requests:** Crafting and sending the prompt to the LLM endpoint.\n",
    "- **Processing Responses:** How to handle and format the LLM’s output.\n",
    "\n",
    "Now you're ready to explore more advanced interactions with LLMs, including chaining prompts and integrating with agents. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
