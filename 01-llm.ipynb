{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Large Language Models (LLMs), Prompts and Tools\n",
    "\n",
    "Welcome to this tutorial on building a simple LLM-powered agent in Python! In this notebook, we will explore the fundamentals of Large Language Models (LLMs) and prompt engineering. The focus will be on understanding how LLMs work, what prompts are, and how to effectively communicate with LLMs to achieve the best possible responses.\n",
    "\n",
    "By the end of this tutorial, you'll understand how to configure a simple LLM, craft effective prompts, and make your first call to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Prompts\n",
    "\n",
    "### What is a Large Language Model (LLM)?\n",
    "\n",
    "A Large Language Model (LLM) is an advanced AI model designed to understand and generate human-like text. LLMs are trained on vast amounts of data and can be used for tasks like answering questions, summarizing information, generating content, and more.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to guide its response. It consists of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the context, role, or instructions for the model.\n",
    "- **User Prompt:** This is the specific request or question the user wants the LLM to address.\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response. Let's break down some key terms used in working with LLMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "![image](images/llm_flow.png)\n",
    "\n",
    "1. **Define Request**: The user defines their request to the LLM.\n",
    "2. **Payload Preparation**\n",
    "\t* **System Prompt**: Sets the role or behavior of the LLM.\n",
    "\t* **User Prompt**: Specifies the question or task to be solved.\n",
    "\t* **Parameters**: Additional settings for fine-tuning the response (e.g., temperature, stop sequences).\n",
    "3. **Send Request to LLM**: The prepared payload is sent to the LLM for processing.\n",
    "4. **LLM Internal Processing**\n",
    "\t* **Tokenization**: Breaks down input into smaller parts (tokens).\n",
    "\t* **Inference/Computation**: Computes a response based on input tokens.\n",
    "\t* **Detokenization**: Converts output tokens back to human-readable text.\n",
    "\t* **Post-Processing**: Makes final adjustments based on parameters.\n",
    "5. **Receive & Process Response**: The LLM generates and sends the response.\n",
    "6. **Output Result**: The processed response is displayed to the user.\n",
    "7. **End**: The process concludes once the result is delivered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `requests` library to send our prompts to the LLM endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring the LLM\n",
    "\n",
    "Next, we’ll set up a configuration to define which model to use and other parameters like temperature and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:latest', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model(model=\"llama3.1:latest\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "llm_config = setup_llm_model()\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the configuration for the LLM, including the endpoint and parameters like temperature and stop sequence.\n",
    "\n",
    "### Key Parameters in LLM Interaction\n",
    "\n",
    "- **Temperature:** This parameter controls the randomness or creativity of the model's responses. A **higher temperature** (e.g., 0.8 or above) results in more diverse and creative outputs, while a **lower temperature** (e.g., 0.2) generates more predictable and deterministic responses. For instance:\n",
    "  - Low temperature (0.2): More focused and concise answers.\n",
    "  - High temperature (0.8): Responses that may include more creative and less common interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crafting the Prompts\n",
    " \n",
    "In the context of Large Language Models (LLMs), **prompts** are crucial for guiding the model's behavior and obtaining useful outputs. They consist of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system and user prompts\n",
    "sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>\"\n",
    "\n",
    "user_request = \"What are the key features of Large Language Models?\"\n",
    "user_prompt = (\n",
    "    f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3.1:latest', 'prompt': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key features of Large Language Models?<|eot_id|>', 'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>', 'temperature': 0.0, 'stop': None, 'stream': False}\n"
     ]
    }
   ],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, config: dict):\n",
    "    return {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"stop\": config[\"stop\"],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=sys_prompt, config=llm_config\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload contains the system and user prompts along with the LLM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making a Call to the LLM\n",
    "\n",
    "Now that we have the prompts prepared, we’ll send the request to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:latest', 'created_at': '2024-09-11T17:43:11.467747Z', 'response': 'Large Language Models (LLMs) have several key features that enable them to process and generate human-like language:\\n\\n1. **Self-Supervised Learning**: LLMs learn from vast amounts of text data without explicit supervision, using techniques like masked language modeling or next sentence prediction.\\n2. **Transformer Architecture**: Most popular LLMs are based on the Transformer architecture, which allows for parallel processing of input sequences and enables efficient computation.\\n3. **Multilayer Perceptron (MLP) Layers**: LLMs typically consist of a series of MLP layers with self-attention mechanisms that weigh inputs according to their relevance.\\n4. **Self-Attention Mechanisms**: These allow the model to focus on specific parts of the input sequence when making predictions, emulating human language processing.\\n5. **Pretraining and Fine-tuning**: LLMs are pre-trained on large datasets and can then be fine-tuned for a specific task or domain.\\n6. **Scalability**: LLMs can process sequences of varying lengths, from short text fragments to long documents.\\n7. **Contextual Understanding**: They capture contextual relationships between words, phrases, and sentences, enabling the model to understand nuances and subtleties in language.\\n8. **Generative Capabilities**: Many LLMs have generative capabilities, allowing them to produce coherent and contextually relevant text.\\n\\nSome popular examples of Large Language Models include:\\n\\n1. BERT (Bidirectional Encoder Representations from Transformers)\\n2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\\n3. XLNet\\n4. Longformer (Long-range Transformers for Automatic Speech Recognition)\\n5. T5 (Text-to-Text Transfer Transformer)\\n\\nThese models have achieved state-of-the-art performance in various natural language processing tasks, such as text classification, question answering, sentiment analysis, and machine translation.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 128000, 128006, 9125, 128007, 2675, 527, 264, 11190, 18328, 430, 5825, 64694, 323, 13687, 11503, 13, 128009, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 35353, 11688, 27972, 320, 4178, 22365, 8, 617, 3892, 1401, 4519, 430, 7431, 1124, 311, 1920, 323, 7068, 3823, 12970, 4221, 1473, 16, 13, 3146, 12363, 6354, 455, 79090, 21579, 96618, 445, 11237, 82, 4048, 505, 13057, 15055, 315, 1495, 828, 2085, 11720, 38217, 11, 1701, 12823, 1093, 43248, 4221, 34579, 477, 1828, 11914, 20212, 627, 17, 13, 3146, 47458, 38943, 96618, 7648, 5526, 445, 11237, 82, 527, 3196, 389, 279, 63479, 18112, 11, 902, 6276, 369, 15638, 8863, 315, 1988, 24630, 323, 20682, 11297, 35547, 627, 18, 13, 3146, 41504, 321, 1155, 3700, 346, 95810, 320, 2735, 47, 8, 84922, 96618, 445, 11237, 82, 11383, 6824, 315, 264, 4101, 315, 95991, 13931, 449, 659, 12, 54203, 24717, 430, 17988, 11374, 4184, 311, 872, 41961, 627, 19, 13, 3146, 12363, 12, 70429, 28901, 13978, 96618, 4314, 2187, 279, 1646, 311, 5357, 389, 3230, 5596, 315, 279, 1988, 8668, 994, 3339, 20492, 11, 991, 15853, 3823, 4221, 8863, 627, 20, 13, 3146, 4808, 31754, 323, 31253, 2442, 38302, 96618, 445, 11237, 82, 527, 864, 70024, 389, 3544, 30525, 323, 649, 1243, 387, 7060, 2442, 49983, 369, 264, 3230, 3465, 477, 8106, 627, 21, 13, 3146, 3407, 278, 2968, 96618, 445, 11237, 82, 649, 1920, 24630, 315, 29865, 29416, 11, 505, 2875, 1495, 35603, 311, 1317, 9477, 627, 22, 13, 3146, 2014, 940, 46551, 96618, 2435, 12602, 66251, 12135, 1990, 4339, 11, 32847, 11, 323, 23719, 11, 28462, 279, 1646, 311, 3619, 84889, 323, 42129, 1169, 552, 304, 4221, 627, 23, 13, 3146, 5648, 1413, 8171, 8623, 96618, 9176, 445, 11237, 82, 617, 1803, 1413, 17357, 11, 10923, 1124, 311, 8356, 56887, 323, 2317, 1870, 9959, 1495, 382, 8538, 5526, 10507, 315, 20902, 11688, 27972, 2997, 1473, 16, 13, 426, 3481, 320, 66552, 45770, 56215, 22717, 811, 505, 81632, 340, 17, 13, 12093, 62537, 64, 320, 14804, 592, 398, 31197, 1534, 426, 3481, 5075, 31754, 54184, 340, 18, 13, 30981, 7099, 198, 19, 13, 5843, 35627, 320, 6720, 31608, 81632, 369, 35081, 39841, 48698, 340, 20, 13, 350, 20, 320, 1199, 4791, 12, 1199, 24078, 63479, 696, 9673, 4211, 617, 17427, 1614, 8838, 10826, 38921, 5178, 304, 5370, 5933, 4221, 8863, 9256, 11, 1778, 439, 1495, 24790, 11, 3488, 36864, 11, 27065, 6492, 11, 323, 5780, 14807, 13], 'total_duration': 7264624750, 'load_duration': 33978584, 'prompt_eval_count': 47, 'prompt_eval_duration': 160275000, 'eval_count': 377, 'eval_duration': 7069473000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_request_to_llm(payload: dict, config: dict):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config[\"model_endpoint\"],\n",
    "            headers=config[\"headers\"],\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Send the request\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prompt to the LLM and handles any errors that might occur during the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the LLM Response\n",
    "\n",
    "Finally, let's process the response and display the relevant information in a user-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed response: Large Language Models (LLMs) have several key features that enable them to process and generate human-like language:\n",
      "\n",
      "1. **Self-Supervised Learning**: LLMs learn from vast amounts of text data without explicit supervision, using techniques like masked language modeling or next sentence prediction.\n",
      "2. **Transformer Architecture**: Most popular LLMs are based on the Transformer architecture, which allows for parallel processing of input sequences and enables efficient computation.\n",
      "3. **Multilayer Perceptron (MLP) Layers**: LLMs typically consist of a series of MLP layers with self-attention mechanisms that weigh inputs according to their relevance.\n",
      "4. **Self-Attention Mechanisms**: These allow the model to focus on specific parts of the input sequence when making predictions, emulating human language processing.\n",
      "5. **Pretraining and Fine-tuning**: LLMs are pre-trained on large datasets and can then be fine-tuned for a specific task or domain.\n",
      "6. **Scalability**: LLMs can process sequences of varying lengths, from short text fragments to long documents.\n",
      "7. **Contextual Understanding**: They capture contextual relationships between words, phrases, and sentences, enabling the model to understand nuances and subtleties in language.\n",
      "8. **Generative Capabilities**: Many LLMs have generative capabilities, allowing them to produce coherent and contextually relevant text.\n",
      "\n",
      "Some popular examples of Large Language Models include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
      "3. XLNet\n",
      "4. Longformer (Long-range Transformers for Automatic Speech Recognition)\n",
      "5. T5 (Text-to-Text Transfer Transformer)\n",
      "\n",
      "These models have achieved state-of-the-art performance in various natural language processing tasks, such as text classification, question answering, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "def process_response(response: dict):\n",
    "    if \"error\" in response:\n",
    "        return f\"Error: {response['error']}\"\n",
    "    return response.get(\"response\", \"No response from the model\")\n",
    "\n",
    "# Process the LLM response\n",
    "processed_response = process_response(response)\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "In this section, we will explore how the LLM can use external tools to enhance its capabilities. We'll begin by adding and configuring a search tool to allow the LLM to perform real-time searches.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **User Submits a Prompt**:\n",
    "   - The process starts when the user provides input or a prompt, which is sent to the **Executor** for handling.\n",
    "\n",
    "2. **Executor Forwards Prompt to LLM**:\n",
    "   - The **Executor** forwards the user’s prompt to the **Llama Model** (LLM) for initial processing.\n",
    "\n",
    "3. **LLM Processes the Prompt**:\n",
    "   - The **Llama Model** analyzes the prompt and determines whether it can generate a response directly or if external tools are needed to complete the request.\n",
    "\n",
    "4. **LLM Requests External Tool Invocation**:\n",
    "   - If external data or additional resources are required (e.g., real-time data from a search engine or a code interpreter), the **Llama Model** signals the **Executor** to call the relevant tool.\n",
    "\n",
    "5. **Executor Calls the Tools**:\n",
    "   - The **Executor** initiates the tool request, calling external tools (such as Brave Search, Wolfram Alpha, Code Interpreter, or other custom tools) to fetch necessary information.\n",
    "\n",
    "6. **Tools Retrieve Data**:\n",
    "   - The external tools access the required information from outside environments (e.g., fetching real-time search results or performing complex calculations) and return the data to the **Executor**.\n",
    "\n",
    "7. **Executor Synthesizes Final Response**:\n",
    "   - The **Executor** combines the original prompt, the **Llama Model**'s processing, and any external tool responses. The final response is then sent back to the **Llama Model**.\n",
    "\n",
    "8. **LLM Generates Final Response**:\n",
    "   - The **Llama Model** synthesizes all the gathered data and generates a final response.\n",
    "\n",
    "9. **Executor Sends Response to User**:\n",
    "   - The **Executor** delivers the final response back to the user, completing the interaction.\n",
    "\n",
    "![image](images/llm_tools_llama.jpeg)\n",
    "Source: [Llama 3.1 Instruct](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change to use an instruct model\n",
    "\n",
    "To better align the LLM’s behavior with task-based instructions, we will switch to an \"instruct\" model, which is optimized for handling more directive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:8b-instruct-fp16', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "# Example configuration\n",
    "llm_config = setup_llm_model(model=\"llama3.1:8b-instruct-fp16\")\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DuckDuckGo Search Tool\n",
    "\n",
    "### Installing the DuckDuckGo Search Tool\n",
    "\n",
    "To enable the LLM to perform real-time searches, we need to install the `duckduckgo-search` library using the `langchain_community` package. This will allow the agent to search the web using DuckDuckGo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain_community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.2.16)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.2.38)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.1.110)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.25.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the DuckDuckGo Search Tool\n",
    "\n",
    "After installation, we will configure the tool by initializing the DuckDuckGo search functionality. This will be the primary tool for real-time search queries in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Tool Name: duckduckgo_search\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize DuckDuckGo Search Tool\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Verify tool name\n",
    "print(\"Search Tool Name:\", duckduckgo_search.name)\n",
    "\n",
    "# Adding the search tool to the list of available tools\n",
    "tools = [duckduckgo_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Name:\n",
      " duckduckgo_search\n",
      "Tools Description:\n",
      " duckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n"
     ]
    }
   ],
   "source": [
    "# Render tool description to ensure it's ready for LLM usage\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "tools_name = duckduckgo_search.name\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "print(\"Tools Name:\\n\", tools_name)\n",
    "print(\"Tools Description:\\n\", tools_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modifying the System Prompt for Tool Integration\n",
    "\n",
    "Now that the DuckDuckGo search tool is set up, we need to rewrite the system prompt to provide tool usage instructions. The LLM will reference this prompt when deciding whether to call the tool.\n",
    "\n",
    "### Updated System Prompt:\n",
    "\n",
    "The updated system prompt provides tool access instructions, explaining how to call the tool and formatting function calls as needed. This ensures that the LLM understands when and how to interact with the DuckDuckGo search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated System Prompt with Tool Instructions\n",
    "sys_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: {tools_name}\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today's Date: 23 July 2024\n",
    "\n",
    "---\n",
    "\n",
    "You are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "You have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "# Tool Instructions\n",
    "- Always use the available tools when asked for real-time or updated information.\n",
    "- If you choose to call a function ONLY reply in the following format:\n",
    "\n",
    "{{\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Example:\n",
    "\n",
    "{{\n",
    "  \"action\": \"duckduckgo_search\",\n",
    "  \"action_input\": {{\n",
    "    \"query\": \"Key features of large language models\"\n",
    "  }} \n",
    "}} \n",
    "\n",
    "Reminder:\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "\n",
    "You are a helpful assistant.<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the System Prompt\n",
    "\n",
    "Now we will insert the tool descriptions and finalize the system prompt by formatting it to include the correct tool name and description. This prompt will guide the LLM in calling the DuckDuckGo search tool when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sys_prompt = sys_prompt.format(\n",
    "    tools_name=tools_name, tools_description=tools_description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. New Request\n",
    "\n",
    "We will now make a new request, asking the LLM to search for the latest trends related to Large Language Models. This time, the LLM will have the ability to call the DuckDuckGo search tool to retrieve real-time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:8b-instruct-fp16', 'created_at': '2024-09-11T17:43:42.549941Z', 'response': '{\\n  \"action\": \"duckduckgo_search\",\\n  \"action_input\": {\\n    \"query\": \"Key features of large language models\"\\n  }\\n}', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 1432, 128000, 128006, 9125, 128007, 271, 13013, 25, 6125, 27993, 198, 16992, 25, 37085, 74070, 3427, 10947, 198, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 596, 2696, 25, 220, 1419, 5887, 220, 2366, 19, 271, 45464, 2675, 527, 459, 25530, 18328, 6319, 311, 3790, 5370, 9256, 11, 2737, 36864, 4860, 11, 8405, 70022, 11, 323, 16785, 11944, 29060, 13, 2052, 16674, 2011, 26549, 387, 304, 4823, 3645, 382, 45464, 567, 14173, 198, 2675, 617, 2680, 311, 264, 8205, 315, 7526, 311, 7945, 304, 27666, 9256, 13, 1472, 527, 8647, 369, 26679, 279, 8475, 8668, 315, 5507, 10648, 311, 1464, 1523, 6485, 9256, 1139, 1207, 25792, 994, 5995, 382, 791, 2561, 7526, 2997, 1473, 74070, 74070, 3427, 10947, 482, 362, 13564, 2212, 46870, 35, 1983, 11087, 7694, 13, 51612, 369, 994, 499, 1205, 311, 4320, 4860, 922, 1510, 4455, 13, 5688, 1288, 387, 264, 2778, 3319, 2637, 2897, 25, 5991, 6, 1663, 1232, 5991, 6, 2150, 1232, 364, 2929, 518, 364, 4789, 1232, 364, 1874, 3319, 311, 1427, 709, 518, 364, 1337, 1232, 364, 928, 23742, 48549, 2, 13782, 39397, 198, 12, 24119, 1005, 279, 2561, 7526, 994, 4691, 369, 1972, 7394, 477, 6177, 2038, 627, 12, 1442, 499, 5268, 311, 1650, 264, 734, 27785, 10052, 304, 279, 2768, 3645, 1473, 517, 220, 330, 1335, 794, 330, 71152, 279, 5507, 499, 1390, 311, 1005, 10560, 220, 330, 1335, 6022, 794, 314, 674, 40665, 2764, 4823, 1988, 369, 279, 1957, 11, 23391, 433, 9248, 279, 5507, 753, 3685, 3645, 323, 828, 4595, 627, 262, 330, 798, 794, 330, 1150, 11374, 311, 279, 5507, 304, 2764, 4823, 3645, 10246, 220, 457, 633, 13617, 1473, 517, 220, 330, 1335, 794, 330, 74070, 74070, 3427, 10947, 761, 220, 330, 1335, 6022, 794, 341, 262, 330, 1663, 794, 330, 1622, 4519, 315, 3544, 4221, 4211, 702, 220, 335, 720, 92, 4815, 96459, 512, 12, 3234, 539, 2997, 5217, 11408, 1778, 439, 1595, 2150, 7964, 1595, 4789, 7964, 477, 1595, 1337, 63, 304, 279, 1595, 14506, 6022, 4077, 12, 5830, 6880, 28832, 1833, 279, 5300, 3645, 198, 12, 12948, 5137, 28832, 387, 5300, 198, 12, 8442, 1650, 832, 734, 520, 264, 892, 198, 12, 10435, 279, 4553, 734, 1650, 10052, 389, 832, 1584, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 198, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 517, 220, 330, 1335, 794, 330, 74070, 74070, 3427, 10947, 761, 220, 330, 1335, 6022, 794, 341, 262, 330, 1663, 794, 330, 1622, 4519, 315, 3544, 4221, 4211, 702, 220, 457, 92], 'total_duration': 2075664875, 'load_duration': 31986916, 'prompt_eval_count': 408, 'prompt_eval_duration': 492885000, 'eval_count': 33, 'eval_duration': 1549730000}\n",
      "Processed response: {\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"Key features of large language models\"\n",
      "  } \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_request = \"What are the most recent Large Language Models?\"\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=formatted_sys_prompt, config=llm_config\n",
    ")\n",
    "\n",
    "processed_response = process_response(response)\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)\n",
    "\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Processing Tool Actions\n",
    "\n",
    "After the LLM provides its response, we will process the action request. If the LLM calls the DuckDuckGo search tool, we will execute the function and retrieve the results. The tool's output will then be formatted and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckduckgo_search\n",
      "<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "Large Language Models (LLMs) have found extensive applications across various domains, revolutionizing how we interact with and process natural language. Some key applications include: Table 1: Key Features of Large Language Models. Feature Description; Versatility: Ability to perform a wide range of language-related tasks: Contextual Understanding: Understanding and generating text based on context and user intent: Scalability: Can be scaled to handle vast amounts of data and complex computations: Examples Included. Large language models (LLMs) understand and generate human-like text. They learn from vast amounts of data and spot patterns in language so they understand context and produce outcomes based on that information. You can use LLM software to write text, personalize messaging, or automate customer interactions. A large language model (LLM) is an artificial intelligence (AI) algorithm trained on large amounts of text data to create natural language outputs. These models have become increasingly popular because they can generate text that sounds just as legitimate as a human would write. Continue reading to learn more about large language models, how ... Large language models (LLM) power a growing number of AI tools, including ChatGPT, Bard, and AI agents, which are the next generation of customer service bots.Now, chatbots powered by LLMs are more than mere hype—they're essential for keeping pace as AI advances. According to Fortune Business Insights, the gen AI market is valued at $43.8 billion, with CX automation among its most ...<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "response_dict = json.loads(processed_response)\n",
    "action = response_dict[\"action\"]\n",
    "action_input = response_dict[\"action_input\"]\n",
    "\n",
    "for tool in tools:\n",
    "    if tool.name == action:\n",
    "        print(tool.name)\n",
    "        try:\n",
    "            result = tool.invoke(action_input)\n",
    "            result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{result}<|eot_id|>\"\"\"\n",
    "            print(result_message)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing tool {action}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Tool {action} not found or unsupported operation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function ensures that we handle any errors gracefully and return the model's response in a readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    " \n",
    "In this notebook, we've successfully extended the capabilities of the LLM by integrating an external tool (DuckDuckGo search). We covered:\n",
    "\n",
    "- **LLM and Tool Integration:** How to modify the system prompt for tool access.\n",
    "- **Tool Setup and Configuration:** Setting up the DuckDuckGo search tool for real-time information retrieval.\n",
    "- **Making Requests and Handling Responses:** Using the tool during LLM interactions and processing the results.\n",
    "\n",
    "With these tools in place, you're now equipped to extend the LLM's capabilities even further by adding more tools and refining its behavior. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
