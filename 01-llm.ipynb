{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple LLM-Powered Agent in Python\n",
    "\n",
    "Welcome to this tutorial on building a simple agent in Python! In this notebook, we will explore the basics of Large Language Models (LLMs) and prompts. The focus will be on understanding LLMs, user prompts, system prompts, and how to communicate with LLMs effectively. \n",
    "\n",
    "By the end of this tutorial, you'll understand how to configure a simple LLM, craft effective prompts, and make your first call to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Prompts\n",
    "\n",
    "### What is a Large Language Model (LLM)?\n",
    "\n",
    "A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human-like text. These models are trained on vast amounts of data and can be used for tasks like answering questions, summarizing information, generating content, and more.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to get a desired output. It typically consists of two parts:\n",
    "\n",
    "- **System Prompt:** This defines the context, role, or instructions for the model.\n",
    "- **User Prompt:** This is the specific request or question that the user wants the LLM to answer.\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "- **System Prompt:** Defines the role or behavior of the LLM.\n",
    "- **User Prompt:** The specific input provided by the user.\n",
    "- **Temperature:** Controls the randomness of the model's output. Higher values produce more creative responses, while lower values produce more deterministic ones.\n",
    "- **Stop Sequence:** Instructs the model where to stop generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "1. **Start:** The interaction begins with defining the task.\n",
    "2. **Crafting Prompts:** The user creates two types of prompts:\n",
    "   - **System Prompt:** Defines the role or behavior of the LLM (e.g., an assistant, a teacher, etc.).\n",
    "   - **User Prompt:** The user's actual question or request (e.g., \"What are the key features of LLMs?\").\n",
    "3. **Prepare Payload:** The prompts, along with any parameters (e.g., temperature, stop sequences), are packaged into a payload to be sent to the LLM.\n",
    "4. **LLM Interaction:** The payload is sent to the LLM API. This triggers the internal processes of the LLM.\n",
    "5. **LLM Internal Processing (Subgraph):** The internal processes include:\n",
    "   - **Tokenization:** The LLM splits the input text into smaller units (tokens) that it can process.\n",
    "   - **Inference/Computation:** The LLM uses its neural network to compute the output based on the tokens and context.\n",
    "   - **Detokenization:** The LLM converts the generated tokens back into human-readable text.\n",
    "   - **Post-Processing:** Any additional processing (e.g., truncating or adjusting based on temperature or stop sequences).\n",
    "6. **Receive & Process Response:** Once the LLM finishes processing, the response is received and can be formatted or processed further if needed.\n",
    "7. **Output Result:** The final output is presented to the user, displaying the LLM’s response to the original prompt.\n",
    "8. **End:** The process concludes once the result has been delivered.\n",
    "\n",
    "![image](images/llm-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `requests` library to send our prompts to the LLM endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring the LLM\n",
    "\n",
    "Next, we’ll set up a configuration to define which model to use and other parameters like temperature and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:8b-instruct-fp16', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model(model=\"llama3.1:8b-instruct-fp16\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "llm_config = setup_llm_model()\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the configuration for the LLM, including the endpoint and parameters like temperature and stop sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crafting the Prompts\n",
    " \n",
    "In the context of Large Language Models (LLMs), **prompts** are crucial for guiding the model's behavior and obtaining useful outputs. They consist of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system and user prompts\n",
    "sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>\"\n",
    "\n",
    "user_request = \"What are the key features of Large Language Models?\"\n",
    "user_prompt = (\n",
    "    f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3.1:8b-instruct-fp16', 'prompt': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key features of Large Language Models?<|eot_id|>', 'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>', 'temperature': 0.0, 'stop': None, 'stream': False}\n"
     ]
    }
   ],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, config: dict):\n",
    "    return {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"stop\": config[\"stop\"],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=sys_prompt, config=llm_config\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload contains the system and user prompts along with the LLM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making a Call to the LLM\n",
    "\n",
    "Now that we have the prompts prepared, we’ll send the request to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:8b-instruct-fp16', 'created_at': '2024-09-06T15:54:05.764056Z', 'response': \"Large Language Models (LLMs) are a type of artificial intelligence (AI) that have gained significant attention in recent years due to their ability to process and generate human-like language. Some of the key features of LLMs include:\\n\\n1. **Deep Learning Architecture**: LLMs are built using deep learning techniques, specifically Recurrent Neural Networks (RNNs) or Transformers, which enable them to learn complex patterns and relationships in large amounts of text data.\\n2. **Self-Supervised Learning**: LLMs can learn from unlabeled text data through self-supervised learning, allowing them to capture the nuances of language without explicit human labeling.\\n3. **Contextual Understanding**: LLMs have the ability to understand context, nuance, and subtlety in language, enabling them to generate responses that are more relevant and coherent.\\n4. **Language Generation**: LLMs can generate text based on a given prompt or topic, often producing coherent and engaging content.\\n5. **Knowledge Retention**: LLMs can retain vast amounts of knowledge from the data they've been trained on, allowing them to provide accurate information and answer questions on a wide range of topics.\\n6. **Flexibility**: LLMs can be fine-tuned for specific tasks such as language translation, sentiment analysis, text summarization, and more.\\n7. **Scalability**: LLMs can handle large amounts of input data and scale to accommodate various sizes of text inputs, making them suitable for applications like chatbots and virtual assistants.\\n8. **Improved Accuracy**: As the model size increases, so does its accuracy in understanding and generating language, enabling it to perform tasks that were previously difficult or impossible for AI models.\\n9. **Transfer Learning**: LLMs can be fine-tuned on specific tasks with smaller datasets, allowing them to leverage pre-existing knowledge from larger models.\\n\\nSome notable examples of Large Language Models include:\\n\\n* BERT (Bidirectional Encoder Representations from Transformers)\\n* RoBERTa\\n* XLNet\\n* transformer-XL\\n* Megatron-LM\\n\\nThese features and applications have led to the development of various industries and use cases, such as language translation, content generation, chatbots, and virtual assistants.\", 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 128000, 128006, 9125, 128007, 2675, 527, 264, 11190, 18328, 430, 5825, 64694, 323, 13687, 11503, 13, 128009, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 35353, 11688, 27972, 320, 4178, 22365, 8, 527, 264, 955, 315, 21075, 11478, 320, 15836, 8, 430, 617, 18661, 5199, 6666, 304, 3293, 1667, 4245, 311, 872, 5845, 311, 1920, 323, 7068, 3823, 12970, 4221, 13, 4427, 315, 279, 1401, 4519, 315, 445, 11237, 82, 2997, 1473, 16, 13, 3146, 34564, 21579, 38943, 96618, 445, 11237, 82, 527, 5918, 1701, 5655, 6975, 12823, 11, 11951, 4158, 963, 61577, 39810, 320, 49, 9944, 82, 8, 477, 81632, 11, 902, 7431, 1124, 311, 4048, 6485, 12912, 323, 12135, 304, 3544, 15055, 315, 1495, 828, 627, 17, 13, 3146, 12363, 6354, 455, 79090, 21579, 96618, 445, 11237, 82, 649, 4048, 505, 65985, 23121, 1495, 828, 1555, 659, 59615, 79090, 6975, 11, 10923, 1124, 311, 12602, 279, 84889, 315, 4221, 2085, 11720, 3823, 55402, 627, 18, 13, 3146, 2014, 940, 46551, 96618, 445, 11237, 82, 617, 279, 5845, 311, 3619, 2317, 11, 11148, 685, 11, 323, 42129, 1169, 88, 304, 4221, 11, 28462, 1124, 311, 7068, 14847, 430, 527, 810, 9959, 323, 56887, 627, 19, 13, 3146, 14126, 24367, 96618, 445, 11237, 82, 649, 7068, 1495, 3196, 389, 264, 2728, 10137, 477, 8712, 11, 3629, 17843, 56887, 323, 23387, 2262, 627, 20, 13, 3146, 81434, 10608, 3012, 96618, 445, 11237, 82, 649, 14389, 13057, 15055, 315, 6677, 505, 279, 828, 814, 3077, 1027, 16572, 389, 11, 10923, 1124, 311, 3493, 13687, 2038, 323, 4320, 4860, 389, 264, 7029, 2134, 315, 13650, 627, 21, 13, 3146, 32771, 3225, 96618, 445, 11237, 82, 649, 387, 7060, 2442, 49983, 369, 3230, 9256, 1778, 439, 4221, 14807, 11, 27065, 6492, 11, 1495, 29385, 2065, 11, 323, 810, 627, 22, 13, 3146, 3407, 278, 2968, 96618, 445, 11237, 82, 649, 3790, 3544, 15055, 315, 1988, 828, 323, 5569, 311, 29376, 5370, 12562, 315, 1495, 11374, 11, 3339, 1124, 14791, 369, 8522, 1093, 6369, 63005, 323, 4200, 57619, 627, 23, 13, 3146, 82210, 51275, 96618, 1666, 279, 1646, 1404, 12992, 11, 779, 1587, 1202, 13708, 304, 8830, 323, 24038, 4221, 11, 28462, 433, 311, 2804, 9256, 430, 1051, 8767, 5107, 477, 12266, 369, 15592, 4211, 627, 24, 13, 3146, 22737, 21579, 96618, 445, 11237, 82, 649, 387, 7060, 2442, 49983, 389, 3230, 9256, 449, 9333, 30525, 11, 10923, 1124, 311, 33164, 864, 50457, 6677, 505, 8294, 4211, 382, 8538, 28289, 10507, 315, 20902, 11688, 27972, 2997, 1473, 9, 426, 3481, 320, 66552, 45770, 56215, 22717, 811, 505, 81632, 340, 9, 12093, 62537, 64, 198, 9, 30981, 7099, 198, 9, 43678, 31650, 43, 198, 9, 28443, 81189, 8288, 44, 271, 9673, 4519, 323, 8522, 617, 6197, 311, 279, 4500, 315, 5370, 19647, 323, 1005, 5157, 11, 1778, 439, 4221, 14807, 11, 2262, 9659, 11, 6369, 63005, 11, 323, 4200, 57619, 13], 'total_duration': 27279205625, 'load_duration': 4582983583, 'prompt_eval_count': 47, 'prompt_eval_duration': 145292000, 'eval_count': 451, 'eval_duration': 22548883000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_request_to_llm(payload: dict, config: dict):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config[\"model_endpoint\"],\n",
    "            headers=config[\"headers\"],\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Send the request\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prompt to the LLM and handles any errors that might occur during the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the LLM Response\n",
    "\n",
    "Finally, let's process the response and display the relevant information in a user-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed response: Large Language Models (LLMs) are a type of artificial intelligence (AI) that have gained significant attention in recent years due to their ability to process and generate human-like language. Some of the key features of LLMs include:\n",
      "\n",
      "1. **Deep Learning Architecture**: LLMs are built using deep learning techniques, specifically Recurrent Neural Networks (RNNs) or Transformers, which enable them to learn complex patterns and relationships in large amounts of text data.\n",
      "2. **Self-Supervised Learning**: LLMs can learn from unlabeled text data through self-supervised learning, allowing them to capture the nuances of language without explicit human labeling.\n",
      "3. **Contextual Understanding**: LLMs have the ability to understand context, nuance, and subtlety in language, enabling them to generate responses that are more relevant and coherent.\n",
      "4. **Language Generation**: LLMs can generate text based on a given prompt or topic, often producing coherent and engaging content.\n",
      "5. **Knowledge Retention**: LLMs can retain vast amounts of knowledge from the data they've been trained on, allowing them to provide accurate information and answer questions on a wide range of topics.\n",
      "6. **Flexibility**: LLMs can be fine-tuned for specific tasks such as language translation, sentiment analysis, text summarization, and more.\n",
      "7. **Scalability**: LLMs can handle large amounts of input data and scale to accommodate various sizes of text inputs, making them suitable for applications like chatbots and virtual assistants.\n",
      "8. **Improved Accuracy**: As the model size increases, so does its accuracy in understanding and generating language, enabling it to perform tasks that were previously difficult or impossible for AI models.\n",
      "9. **Transfer Learning**: LLMs can be fine-tuned on specific tasks with smaller datasets, allowing them to leverage pre-existing knowledge from larger models.\n",
      "\n",
      "Some notable examples of Large Language Models include:\n",
      "\n",
      "* BERT (Bidirectional Encoder Representations from Transformers)\n",
      "* RoBERTa\n",
      "* XLNet\n",
      "* transformer-XL\n",
      "* Megatron-LM\n",
      "\n",
      "These features and applications have led to the development of various industries and use cases, such as language translation, content generation, chatbots, and virtual assistants.\n"
     ]
    }
   ],
   "source": [
    "def process_response(response: dict):\n",
    "    if \"error\" in response:\n",
    "        return f\"Error: {response['error']}\"\n",
    "    return response.get(\"response\", \"No response from the model\")\n",
    "\n",
    "# Process the LLM response\n",
    "processed_response = process_response(response)\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function ensures that we handle any errors gracefully and return the model's response in a readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've simplified the process of interacting with an LLM. We've covered:\n",
    "- **LLMs and Prompts:** Understanding system and user prompts.\n",
    "- **Model Configuration:** How to configure the LLM for use.\n",
    "- **Sending Requests:** Crafting and sending the prompt to the LLM endpoint.\n",
    "- **Processing Responses:** How to handle and format the LLM’s output.\n",
    "\n",
    "Now you're ready to explore more advanced interactions with LLMs, including chaining prompts and integrating with agents. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
