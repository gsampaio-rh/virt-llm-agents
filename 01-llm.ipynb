{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple LLM-Powered Agent in Python\n",
    "\n",
    "Welcome to this tutorial on building a simple LLM-powered agent in Python! In this notebook, we will explore the fundamentals of Large Language Models (LLMs) and prompt engineering. The focus will be on understanding how LLMs work, what prompts are, and how to effectively communicate with LLMs to achieve the best possible responses.\n",
    "\n",
    "By the end of this tutorial, you'll understand how to configure a simple LLM, craft effective prompts, and make your first call to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Prompts\n",
    "\n",
    "### What is a Large Language Model (LLM)?\n",
    "\n",
    "A Large Language Model (LLM) is an advanced AI model designed to understand and generate human-like text. LLMs are trained on vast amounts of data and can be used for tasks like answering questions, summarizing information, generating content, and more.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to guide its response. It consists of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the context, role, or instructions for the model.\n",
    "- **User Prompt:** This is the specific request or question the user wants the LLM to address.\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response. Let's break down some key terms used in working with LLMs:\n",
    "\n",
    "### Key Parameters in LLM Interaction\n",
    "\n",
    "- **Temperature:** This parameter controls the randomness or creativity of the model's responses. A **higher temperature** (e.g., 0.8 or above) results in more diverse and creative outputs, while a **lower temperature** (e.g., 0.2) generates more predictable and deterministic responses. For instance:\n",
    "  - Low temperature (0.2): More focused and concise answers.\n",
    "  - High temperature (0.8): Responses that may include more creative and less common interpretations.\n",
    "- **Top-p:** Also known as **nucleus sampling**, this parameter controls the cumulative probability threshold for generating responses. For example, if `top_p = 0.9`, the model will sample from the smallest possible set of words whose combined probability is at least 90%. This encourages more coherent responses by focusing on high-probability word choices.\n",
    "- **Top-k:** This parameter limits the number of highest-probability tokens the model can choose from during generation. If `top_k = 50`, the model will sample from the top 50 tokens instead of considering the entire vocabulary, which can help balance creativity and relevance.\n",
    "- **Repetition Penalty:** This is a penalty applied to repeated tokens during text generation. A higher repetition penalty discourages the model from repeating phrases or words too often, leading to more varied responses. This is useful when you want to avoid repetitive or redundant outputs in generated text.\n",
    "- **Stop Sequence:** The stop sequence tells the model when to stop generating text. It is useful when you want to prevent the model from continuing beyond a certain point. For example, if your output should end after a specific sentence or character, you can define a stop sequence like a newline (`\\n`) or a punctuation mark (`.`).\n",
    "\n",
    "By understanding these elements, you'll be able to guide the LLM effectively and obtain the most useful and relevant responses for your tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "1. **Define Request**: The user defines their request to the LLM.\n",
    "2. **Payload Preparation**\n",
    "\t* **System Prompt**: Sets the role or behavior of the LLM.\n",
    "\t* **User Prompt**: Specifies the question or task to be solved.\n",
    "\t* **Parameters**: Additional settings for fine-tuning the response (e.g., temperature, stop sequences).\n",
    "3. **Send Request to LLM**: The prepared payload is sent to the LLM for processing.\n",
    "4. **LLM Internal Processing**\n",
    "\t* **Tokenization**: Breaks down input into smaller parts (tokens).\n",
    "\t* **Inference/Computation**: Computes a response based on input tokens.\n",
    "\t* **Detokenization**: Converts output tokens back to human-readable text.\n",
    "\t* **Post-Processing**: Makes final adjustments based on parameters.\n",
    "5. **Receive & Process Response**: The LLM generates and sends the response.\n",
    "6. **Output Result**: The processed response is displayed to the user.\n",
    "7. **End**: The process concludes once the result is delivered.\n",
    "\n",
    "![image](images/llm_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `requests` library to send our prompts to the LLM endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring the LLM\n",
    "\n",
    "Next, we’ll set up a configuration to define which model to use and other parameters like temperature and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:latest', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model(model=\"llama3.1:latest\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "llm_config = setup_llm_model()\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the configuration for the LLM, including the endpoint and parameters like temperature and stop sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crafting the Prompts\n",
    " \n",
    "In the context of Large Language Models (LLMs), **prompts** are crucial for guiding the model's behavior and obtaining useful outputs. They consist of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system and user prompts\n",
    "sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>\"\n",
    "\n",
    "user_request = \"What are the key features of Large Language Models?\"\n",
    "user_prompt = (\n",
    "    f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3.1:latest', 'prompt': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key features of Large Language Models?<|eot_id|>', 'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>', 'temperature': 0.0, 'stop': None, 'stream': False}\n"
     ]
    }
   ],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, config: dict):\n",
    "    return {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"stop\": config[\"stop\"],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=sys_prompt, config=llm_config\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload contains the system and user prompts along with the LLM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making a Call to the LLM\n",
    "\n",
    "Now that we have the prompts prepared, we’ll send the request to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:latest', 'created_at': '2024-09-11T13:25:34.054583Z', 'response': 'The key features of Large Language Models (LLMs) include:\\n\\n1. **Deep Learning Architecture**: LLMs are based on deep learning architectures, typically using Transformer models, which consist of self-attention mechanisms and fully connected layers.\\n2. **Multi-Layer Perceptron (MLP)**: The core component of an LLM is a series of interconnected neural networks (MLPs) that process input text sequences one token at a time.\\n3. **Self-Attention Mechanism**: This mechanism allows the model to weigh importance to different parts of the input sequence and generate context-dependent representations.\\n4. **Parallel Processing**: LLMs can process multiple tokens simultaneously, making them computationally efficient for long input sequences.\\n5. **Contextual Understanding**: LLMs capture contextual relationships between words in a sentence or passage, enabling them to better understand nuances and subtleties of language.\\n6. **Large Vocabulary Size**: LLMs are trained on vast amounts of text data, allowing them to learn from millions of unique tokens (words) and their variations.\\n7. **Generative Capacity**: LLMs can generate coherent text, summarize content, answer questions, or even create original text based on the input provided.\\n8. **Multi-Task Learning**: Many modern LLMs are trained for multiple tasks simultaneously, such as language translation, sentiment analysis, and question answering.\\n9. **Pre-Training and Fine-Tuning**: Most popular LLMs are pre-trained on a large corpus of text data and then fine-tuned for specific downstream tasks.\\n10. **Scalability**: Large Language Models can be scaled up or down depending on the desired computational resources and application requirements.\\n\\nSome notable examples of Large Language Models include:\\n\\n* BERT (Bidirectional Encoder Representations from Transformers)\\n* RoBERTa (Robustly Optimized BERT Pretraining Approach)\\n* XLNet\\n* ALICE (Apprentice Linguistic Intelligence for Conversational Engagement)\\n\\nThese models have revolutionized the field of natural language processing and are now widely used in various applications, including chatbots, sentiment analysis, language translation, text summarization, and more.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 128000, 128006, 9125, 128007, 2675, 527, 264, 11190, 18328, 430, 5825, 64694, 323, 13687, 11503, 13, 128009, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 791, 1401, 4519, 315, 20902, 11688, 27972, 320, 4178, 22365, 8, 2997, 1473, 16, 13, 3146, 34564, 21579, 38943, 96618, 445, 11237, 82, 527, 3196, 389, 5655, 6975, 78335, 11, 11383, 1701, 63479, 4211, 11, 902, 6824, 315, 659, 12, 54203, 24717, 323, 7373, 8599, 13931, 627, 17, 13, 3146, 20981, 8288, 1155, 3700, 346, 95810, 320, 2735, 47, 33395, 25, 578, 6332, 3777, 315, 459, 445, 11237, 374, 264, 4101, 315, 83416, 30828, 14488, 320, 2735, 21051, 8, 430, 1920, 1988, 1495, 24630, 832, 4037, 520, 264, 892, 627, 18, 13, 3146, 12363, 12, 70429, 28901, 2191, 96618, 1115, 17383, 6276, 279, 1646, 311, 17988, 12939, 311, 2204, 5596, 315, 279, 1988, 8668, 323, 7068, 2317, 43918, 44713, 627, 19, 13, 3146, 16956, 29225, 96618, 445, 11237, 82, 649, 1920, 5361, 11460, 25291, 11, 3339, 1124, 3801, 30154, 11297, 369, 1317, 1988, 24630, 627, 20, 13, 3146, 2014, 940, 46551, 96618, 445, 11237, 82, 12602, 66251, 12135, 1990, 4339, 304, 264, 11914, 477, 21765, 11, 28462, 1124, 311, 2731, 3619, 84889, 323, 42129, 1169, 552, 315, 4221, 627, 21, 13, 3146, 35353, 99272, 8645, 96618, 445, 11237, 82, 527, 16572, 389, 13057, 15055, 315, 1495, 828, 11, 10923, 1124, 311, 4048, 505, 11990, 315, 5016, 11460, 320, 5880, 8, 323, 872, 27339, 627, 22, 13, 3146, 5648, 1413, 41210, 96618, 445, 11237, 82, 649, 7068, 56887, 1495, 11, 63179, 2262, 11, 4320, 4860, 11, 477, 1524, 1893, 4113, 1495, 3196, 389, 279, 1988, 3984, 627, 23, 13, 3146, 20981, 12, 6396, 21579, 96618, 9176, 6617, 445, 11237, 82, 527, 16572, 369, 5361, 9256, 25291, 11, 1778, 439, 4221, 14807, 11, 27065, 6492, 11, 323, 3488, 36864, 627, 24, 13, 3146, 4808, 12, 38030, 323, 31253, 9469, 38302, 96618, 7648, 5526, 445, 11237, 82, 527, 864, 70024, 389, 264, 3544, 43194, 315, 1495, 828, 323, 1243, 7060, 2442, 49983, 369, 3230, 52452, 9256, 627, 605, 13, 3146, 3407, 278, 2968, 96618, 20902, 11688, 27972, 649, 387, 31790, 709, 477, 1523, 11911, 389, 279, 12974, 55580, 5070, 323, 3851, 8670, 382, 8538, 28289, 10507, 315, 20902, 11688, 27972, 2997, 1473, 9, 426, 3481, 320, 66552, 45770, 56215, 22717, 811, 505, 81632, 340, 9, 12093, 62537, 64, 320, 14804, 592, 398, 31197, 1534, 426, 3481, 5075, 31754, 54184, 340, 9, 30981, 7099, 198, 9, 8927, 5604, 320, 2213, 8135, 560, 89333, 4633, 22107, 369, 56496, 1697, 60463, 696, 9673, 4211, 617, 14110, 1534, 279, 2115, 315, 5933, 4221, 8863, 323, 527, 1457, 13882, 1511, 304, 5370, 8522, 11, 2737, 6369, 63005, 11, 27065, 6492, 11, 4221, 14807, 11, 1495, 29385, 2065, 11, 323, 810, 13], 'total_duration': 9621722833, 'load_duration': 1368348500, 'prompt_eval_count': 47, 'prompt_eval_duration': 156393000, 'eval_count': 434, 'eval_duration': 8096237000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_request_to_llm(payload: dict, config: dict):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config[\"model_endpoint\"],\n",
    "            headers=config[\"headers\"],\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Send the request\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prompt to the LLM and handles any errors that might occur during the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the LLM Response\n",
    "\n",
    "Finally, let's process the response and display the relevant information in a user-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed response: The key features of Large Language Models (LLMs) include:\n",
      "\n",
      "1. **Deep Learning Architecture**: LLMs are based on deep learning architectures, typically using Transformer models, which consist of self-attention mechanisms and fully connected layers.\n",
      "2. **Multi-Layer Perceptron (MLP)**: The core component of an LLM is a series of interconnected neural networks (MLPs) that process input text sequences one token at a time.\n",
      "3. **Self-Attention Mechanism**: This mechanism allows the model to weigh importance to different parts of the input sequence and generate context-dependent representations.\n",
      "4. **Parallel Processing**: LLMs can process multiple tokens simultaneously, making them computationally efficient for long input sequences.\n",
      "5. **Contextual Understanding**: LLMs capture contextual relationships between words in a sentence or passage, enabling them to better understand nuances and subtleties of language.\n",
      "6. **Large Vocabulary Size**: LLMs are trained on vast amounts of text data, allowing them to learn from millions of unique tokens (words) and their variations.\n",
      "7. **Generative Capacity**: LLMs can generate coherent text, summarize content, answer questions, or even create original text based on the input provided.\n",
      "8. **Multi-Task Learning**: Many modern LLMs are trained for multiple tasks simultaneously, such as language translation, sentiment analysis, and question answering.\n",
      "9. **Pre-Training and Fine-Tuning**: Most popular LLMs are pre-trained on a large corpus of text data and then fine-tuned for specific downstream tasks.\n",
      "10. **Scalability**: Large Language Models can be scaled up or down depending on the desired computational resources and application requirements.\n",
      "\n",
      "Some notable examples of Large Language Models include:\n",
      "\n",
      "* BERT (Bidirectional Encoder Representations from Transformers)\n",
      "* RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
      "* XLNet\n",
      "* ALICE (Apprentice Linguistic Intelligence for Conversational Engagement)\n",
      "\n",
      "These models have revolutionized the field of natural language processing and are now widely used in various applications, including chatbots, sentiment analysis, language translation, text summarization, and more.\n"
     ]
    }
   ],
   "source": [
    "def process_response(response: dict):\n",
    "    if \"error\" in response:\n",
    "        return f\"Error: {response['error']}\"\n",
    "    return response.get(\"response\", \"No response from the model\")\n",
    "\n",
    "# Process the LLM response\n",
    "processed_response = process_response(response)\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "In this section, we will explore how the LLM can use external tools to enhance its capabilities. We'll begin by adding and configuring a search tool to allow the LLM to perform real-time searches.\n",
    "\n",
    "![image](images/llm_tool_flow.png)\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **User Accesses Jupyter Notebook**:\n",
    "   - The interaction begins with the user providing input or a prompt in the Jupyter Notebook environment.\n",
    "   - The notebook acts as the intermediary, handling the user’s request.\n",
    "\n",
    "2. **Notebook Sends Prompt to LLM**:\n",
    "   - Once the user submits their input, the notebook sends the system and user prompts to the LLM. In this example, the LLM resides within a Python environment (e.g., using Ollama for model serving).\n",
    "\n",
    "3. **LLM Processes and Responds**:\n",
    "   - The LLM processes the prompt, generating a response. The LLM might decide if external real-time data or tools are required for the request.\n",
    "\n",
    "4. **LLM Invokes External Tools**:\n",
    "   - If the LLM determines that real-time data is needed (such as search results), it invokes the appropriate tool (e.g., DuckDuckGo) via the Python environment.\n",
    "\n",
    "5. **Tool Accesses External Environment**:\n",
    "   - The tool retrieves data from the external environment (e.g., querying DuckDuckGo for real-time search results). The fetched data is sent back to the notebook for further processing.\n",
    "\n",
    "6. **External Environment Sends Response**:\n",
    "   - The external environment (e.g., a search engine or API) responds to the tool with the necessary data.\n",
    "\n",
    "7. **Notebook Processes Final Response**:\n",
    "   - The notebook receives the response from either the LLM directly (if no external data was needed) or from the tool. The final response is then formatted and returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change to use an instruct model\n",
    "\n",
    "To better align the LLM’s behavior with task-based instructions, we will switch to an \"instruct\" model, which is optimized for handling more directive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:8b-instruct-fp16', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "# Example configuration\n",
    "llm_config = setup_llm_model(model=\"llama3.1:8b-instruct-fp16\")\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DuckDuckGo Search Tool\n",
    "\n",
    "### Installing the DuckDuckGo Search Tool\n",
    "\n",
    "To enable the LLM to perform real-time searches, we need to install the `duckduckgo-search` library using the `langchain_community` package. This will allow the agent to search the web using DuckDuckGo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain_community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.2.16)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.2.38)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.1.110)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.25.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the DuckDuckGo Search Tool\n",
    "\n",
    "After installation, we will configure the tool by initializing the DuckDuckGo search functionality. This will be the primary tool for real-time search queries in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Tool Name: duckduckgo_search\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize DuckDuckGo Search Tool\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Verify tool name\n",
    "print(\"Search Tool Name:\", duckduckgo_search.name)\n",
    "\n",
    "# Adding the search tool to the list of available tools\n",
    "tools = [duckduckgo_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'duckSearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Render tool description to ensure it's ready for LLM usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m render_text_description_and_args\n\u001b[0;32m----> 4\u001b[0m tools_name \u001b[38;5;241m=\u001b[39m \u001b[43mduckSearch\u001b[49m\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m      6\u001b[0m tools_description \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     render_text_description_and_args(tools)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools Name:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, tools_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'duckSearch' is not defined"
     ]
    }
   ],
   "source": [
    "# Render tool description to ensure it's ready for LLM usage\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "tools_name = duckSearch.name\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "print(\"Tools Name:\\n\", tools_name)\n",
    "print(\"Tools Description:\\n\", tools_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modifying the System Prompt for Tool Integration\n",
    "\n",
    "Now that the DuckDuckGo search tool is set up, we need to rewrite the system prompt to provide tool usage instructions. The LLM will reference this prompt when deciding whether to call the tool.\n",
    "\n",
    "### Updated System Prompt:\n",
    "\n",
    "The updated system prompt provides tool access instructions, explaining how to call the tool and formatting function calls as needed. This ensures that the LLM understands when and how to interact with the DuckDuckGo search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated System Prompt with Tool Instructions\n",
    "sys_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: {tools_name}\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today's Date: 23 July 2024\n",
    "\n",
    "---\n",
    "\n",
    "You are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "You have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "# Tool Instructions\n",
    "- Always use the available tools when asked for real-time or updated information.\n",
    "- If you choose to call a function ONLY reply in the following format:\n",
    "\n",
    "{{\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Example:\n",
    "\n",
    "{{\n",
    "  \"action\": \"duckduckgo_search\",\n",
    "  \"action_input\": {{\n",
    "    \"query\": \"Key features of large language models\"\n",
    "  }} \n",
    "}} \n",
    "\n",
    "Reminder:\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "\n",
    "You are a helpful assistant.<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the System Prompt\n",
    "\n",
    "Now we will insert the tool descriptions and finalize the system prompt by formatting it to include the correct tool name and description. This prompt will guide the LLM in calling the DuckDuckGo search tool when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sys_prompt = sys_prompt.format(\n",
    "    tools_name=tools_name, tools_description=tools_description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. New Request\n",
    "\n",
    "We will now make a new request, asking the LLM to search for the latest trends related to Large Language Models. This time, the LLM will have the ability to call the DuckDuckGo search tool to retrieve real-time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_request = \"What are the most recent Large Language Models?\"\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=formatted_sys_prompt, config=llm_config\n",
    ")\n",
    "\n",
    "processed_response = process_response(response)\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)\n",
    "\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Processing Tool Actions\n",
    "\n",
    "After the LLM provides its response, we will process the action request. If the LLM calls the DuckDuckGo search tool, we will execute the function and retrieve the results. The tool's output will then be formatted and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = json.loads(processed_response)\n",
    "action = response_dict[\"action\"]\n",
    "action_input = response_dict[\"action_input\"]\n",
    "\n",
    "for tool in tools:\n",
    "    if tool.name == action:\n",
    "        print(tool.name)\n",
    "        try:\n",
    "            result = tool.invoke(action_input)\n",
    "            result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{result}<|eot_id|>\"\"\"\n",
    "            print(result_message)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing tool {action}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Tool {action} not found or unsupported operation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function ensures that we handle any errors gracefully and return the model's response in a readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    " \n",
    "In this notebook, we've successfully extended the capabilities of the LLM by integrating an external tool (DuckDuckGo search). We covered:\n",
    "\n",
    "- **LLM and Tool Integration:** How to modify the system prompt for tool access.\n",
    "- **Tool Setup and Configuration:** Setting up the DuckDuckGo search tool for real-time information retrieval.\n",
    "- **Making Requests and Handling Responses:** Using the tool during LLM interactions and processing the results.\n",
    "\n",
    "With these tools in place, you're now equipped to extend the LLM's capabilities even further by adding more tools and refining its behavior. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
