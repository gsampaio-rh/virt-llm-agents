{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple LLM-Powered Agent in Python\n",
    "\n",
    "Welcome to this tutorial on building a simple LLM-powered agent in Python! In this notebook, we will explore the fundamentals of Large Language Models (LLMs) and prompt engineering. The focus will be on understanding how LLMs work, what prompts are, and how to effectively communicate with LLMs to achieve the best possible responses.\n",
    "\n",
    "By the end of this tutorial, you'll understand how to configure a simple LLM, craft effective prompts, and make your first call to an LLM.\n",
    "\n",
    "## LLMs and Prompts\n",
    "\n",
    "### What is a Large Language Model (LLM)?\n",
    "\n",
    "A Large Language Model (LLM) is an advanced AI model designed to understand and generate human-like text. LLMs are trained on vast amounts of data and can be used for tasks like answering questions, summarizing information, generating content, and more.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to guide its response. It consists of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the context, role, or instructions for the model.\n",
    "- **User Prompt:** This is the specific request or question the user wants the LLM to address.\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response. Let's break down some key terms used in working with LLMs:\n",
    "\n",
    "### Key Parameters in LLM Interaction\n",
    "\n",
    "- **Temperature:** This parameter controls the randomness or creativity of the model's responses. A **higher temperature** (e.g., 0.8 or above) results in more diverse and creative outputs, while a **lower temperature** (e.g., 0.2) generates more predictable and deterministic responses. For instance:\n",
    "  - Low temperature (0.2): More focused and concise answers.\n",
    "  - High temperature (0.8): Responses that may include more creative and less common interpretations.\n",
    "- **Top-p:** Also known as **nucleus sampling**, this parameter controls the cumulative probability threshold for generating responses. For example, if `top_p = 0.9`, the model will sample from the smallest possible set of words whose combined probability is at least 90%. This encourages more coherent responses by focusing on high-probability word choices.\n",
    "- **Top-k:** This parameter limits the number of highest-probability tokens the model can choose from during generation. If `top_k = 50`, the model will sample from the top 50 tokens instead of considering the entire vocabulary, which can help balance creativity and relevance.\n",
    "- **Repetition Penalty:** This is a penalty applied to repeated tokens during text generation. A higher repetition penalty discourages the model from repeating phrases or words too often, leading to more varied responses. This is useful when you want to avoid repetitive or redundant outputs in generated text.\n",
    "- **Stop Sequence:** The stop sequence tells the model when to stop generating text. It is useful when you want to prevent the model from continuing beyond a certain point. For example, if your output should end after a specific sentence or character, you can define a stop sequence like a newline (`\\n`) or a punctuation mark (`.`).\n",
    "\n",
    "By understanding these elements, you'll be able to guide the LLM effectively and obtain the most useful and relevant responses for your tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "1. **Start:** Define your request to the LLM.\n",
    "2. **Payload Preparation:** \n",
    "   - **System Prompt:** Sets the role or behavior of the LLM.\n",
    "   - **User Prompt:** The specific question or task you want to solve.\n",
    "   - **Parameters:** Additional settings like temperature or stop sequences to fine-tune the response.\n",
    "   These components are packaged together into a payload.\n",
    "3. **Send Request to LLM:** The payload is sent to the LLM for processing.\n",
    "4. **LLM Internal Processing:**\n",
    "   - **Tokenization:** The input is broken down into smaller parts (tokens).\n",
    "   - **Inference/Computation:** The model computes a response based on the input tokens.\n",
    "   - **Detokenization:** The output tokens are converted back to human-readable text.\n",
    "   - **Post-Processing:** Any final adjustments are made based on parameters.\n",
    "5. **Receive & Process Response:** The response is received from the LLM.\n",
    "6. **Output Result:** The processed response is displayed to the user.\n",
    "7. **End:** The process concludes once the result is delivered.\n",
    "\n",
    "![image](images/llm_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `requests` library to send our prompts to the LLM endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring the LLM\n",
    "\n",
    "Next, we’ll set up a configuration to define which model to use and other parameters like temperature and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:latest', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model(model=\"llama3.1:latest\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "llm_config = setup_llm_model()\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the configuration for the LLM, including the endpoint and parameters like temperature and stop sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crafting the Prompts\n",
    " \n",
    "In the context of Large Language Models (LLMs), **prompts** are crucial for guiding the model's behavior and obtaining useful outputs. They consist of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system and user prompts\n",
    "sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>\"\n",
    "\n",
    "user_request = \"What are the key features of Large Language Models?\"\n",
    "user_prompt = (\n",
    "    f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3.1:latest', 'prompt': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key features of Large Language Models?<|eot_id|>', 'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>', 'temperature': 0.0, 'stop': None, 'stream': False}\n"
     ]
    }
   ],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, config: dict):\n",
    "    return {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"stop\": config[\"stop\"],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=sys_prompt, config=llm_config\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload contains the system and user prompts along with the LLM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making a Call to the LLM\n",
    "\n",
    "Now that we have the prompts prepared, we’ll send the request to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:latest', 'created_at': '2024-09-09T14:51:37.915428Z', 'response': 'Large language models, also known as transformer-based models or generative pre-trained transformers (GPT), have several key features that enable them to process and generate human-like text:\\n\\n1. **Self-Attention Mechanism**: This is a crucial component of large language models, allowing the model to focus on specific parts of the input sequence while generating output.\\n2. **Transformer Architecture**: The transformer architecture replaces traditional recurrent neural networks (RNNs) with self-attention mechanisms, making it more efficient and parallelizable.\\n3. **Deep Learning Model**: Large language models are deep learning models that consist of multiple layers, enabling them to capture complex relationships between input and output sequences.\\n4. **Generative Capabilities**: These models can generate new text based on the patterns learned from large datasets, making them useful for tasks like language translation, chatbots, and text summarization.\\n5. **Pre-Training and Fine-Tuning**: Large language models are often pre-trained on vast amounts of text data and then fine-tuned for specific downstream tasks, allowing them to adapt to various applications.\\n6. **Highly Parallelizable**: The transformer architecture makes it easy to parallelize the computation across multiple GPUs or TPUs, significantly improving training speed and efficiency.\\n7. **Handling Long-Range Dependencies**: Large language models can capture long-range dependencies between input tokens, enabling them to understand complex context and relationships in text data.\\n\\nThese features have led to significant advancements in natural language processing (NLP) tasks, including:\\n\\n* Text classification\\n* Language translation\\n* Sentiment analysis\\n* Text summarization\\n* Conversational AI\\n* Dialogue generation', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 128000, 128006, 9125, 128007, 2675, 527, 264, 11190, 18328, 430, 5825, 64694, 323, 13687, 11503, 13, 128009, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 35353, 4221, 4211, 11, 1101, 3967, 439, 43678, 6108, 4211, 477, 1803, 1413, 864, 70024, 87970, 320, 38, 2898, 705, 617, 3892, 1401, 4519, 430, 7431, 1124, 311, 1920, 323, 7068, 3823, 12970, 1495, 1473, 16, 13, 3146, 12363, 12, 70429, 28901, 2191, 96618, 1115, 374, 264, 16996, 3777, 315, 3544, 4221, 4211, 11, 10923, 279, 1646, 311, 5357, 389, 3230, 5596, 315, 279, 1988, 8668, 1418, 24038, 2612, 627, 17, 13, 3146, 47458, 38943, 96618, 578, 43678, 18112, 41800, 8776, 65174, 30828, 14488, 320, 49, 9944, 82, 8, 449, 659, 12, 54203, 24717, 11, 3339, 433, 810, 11297, 323, 15638, 8499, 627, 18, 13, 3146, 34564, 21579, 5008, 96618, 20902, 4221, 4211, 527, 5655, 6975, 4211, 430, 6824, 315, 5361, 13931, 11, 28462, 1124, 311, 12602, 6485, 12135, 1990, 1988, 323, 2612, 24630, 627, 19, 13, 3146, 5648, 1413, 8171, 8623, 96618, 4314, 4211, 649, 7068, 502, 1495, 3196, 389, 279, 12912, 9687, 505, 3544, 30525, 11, 3339, 1124, 5505, 369, 9256, 1093, 4221, 14807, 11, 6369, 63005, 11, 323, 1495, 29385, 2065, 627, 20, 13, 3146, 4808, 12, 38030, 323, 31253, 9469, 38302, 96618, 20902, 4221, 4211, 527, 3629, 864, 70024, 389, 13057, 15055, 315, 1495, 828, 323, 1243, 7060, 2442, 49983, 369, 3230, 52452, 9256, 11, 10923, 1124, 311, 10737, 311, 5370, 8522, 627, 21, 13, 3146, 12243, 398, 50372, 8499, 96618, 578, 43678, 18112, 3727, 433, 4228, 311, 15638, 553, 279, 35547, 4028, 5361, 71503, 477, 30170, 3642, 11, 12207, 18899, 4967, 4732, 323, 15374, 627, 22, 13, 3146, 39706, 5843, 12, 6174, 81590, 96618, 20902, 4221, 4211, 649, 12602, 1317, 31608, 20113, 1990, 1988, 11460, 11, 28462, 1124, 311, 3619, 6485, 2317, 323, 12135, 304, 1495, 828, 382, 9673, 4519, 617, 6197, 311, 5199, 83787, 304, 5933, 4221, 8863, 320, 45, 12852, 8, 9256, 11, 2737, 1473, 9, 2991, 24790, 198, 9, 11688, 14807, 198, 9, 24248, 3904, 6492, 198, 9, 2991, 29385, 2065, 198, 9, 56496, 1697, 15592, 198, 9, 70589, 9659], 'total_duration': 13687806208, 'load_duration': 7147514041, 'prompt_eval_count': 47, 'prompt_eval_duration': 164409000, 'eval_count': 329, 'eval_duration': 6374448000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_request_to_llm(payload: dict, config: dict):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config[\"model_endpoint\"],\n",
    "            headers=config[\"headers\"],\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Send the request\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prompt to the LLM and handles any errors that might occur during the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the LLM Response\n",
    "\n",
    "Finally, let's process the response and display the relevant information in a user-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed response: Large language models, also known as transformer-based models or generative pre-trained transformers (GPT), have several key features that enable them to process and generate human-like text:\n",
      "\n",
      "1. **Self-Attention Mechanism**: This is a crucial component of large language models, allowing the model to focus on specific parts of the input sequence while generating output.\n",
      "2. **Transformer Architecture**: The transformer architecture replaces traditional recurrent neural networks (RNNs) with self-attention mechanisms, making it more efficient and parallelizable.\n",
      "3. **Deep Learning Model**: Large language models are deep learning models that consist of multiple layers, enabling them to capture complex relationships between input and output sequences.\n",
      "4. **Generative Capabilities**: These models can generate new text based on the patterns learned from large datasets, making them useful for tasks like language translation, chatbots, and text summarization.\n",
      "5. **Pre-Training and Fine-Tuning**: Large language models are often pre-trained on vast amounts of text data and then fine-tuned for specific downstream tasks, allowing them to adapt to various applications.\n",
      "6. **Highly Parallelizable**: The transformer architecture makes it easy to parallelize the computation across multiple GPUs or TPUs, significantly improving training speed and efficiency.\n",
      "7. **Handling Long-Range Dependencies**: Large language models can capture long-range dependencies between input tokens, enabling them to understand complex context and relationships in text data.\n",
      "\n",
      "These features have led to significant advancements in natural language processing (NLP) tasks, including:\n",
      "\n",
      "* Text classification\n",
      "* Language translation\n",
      "* Sentiment analysis\n",
      "* Text summarization\n",
      "* Conversational AI\n",
      "* Dialogue generation\n"
     ]
    }
   ],
   "source": [
    "def process_response(response: dict):\n",
    "    if \"error\" in response:\n",
    "        return f\"Error: {response['error']}\"\n",
    "    return response.get(\"response\", \"No response from the model\")\n",
    "\n",
    "# Process the LLM response\n",
    "processed_response = process_response(response)\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "In this section, we will explore how the LLM can use external tools to enhance its capabilities. We'll begin by adding and configuring a search tool to allow the LLM to perform real-time searches.\n",
    "\n",
    "![image](images/llm_tool_flow.png)\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **User Accesses Jupyter Notebook**:\n",
    "   - The interaction begins with the user providing input or a prompt in the Jupyter Notebook environment.\n",
    "   - The notebook acts as the intermediary, handling the user’s request.\n",
    "\n",
    "2. **Notebook Sends Prompt to LLM**:\n",
    "   - Once the user submits their input, the notebook sends the system and user prompts to the LLM. In this example, the LLM resides within a Python environment (e.g., using Ollama for model serving).\n",
    "\n",
    "3. **LLM Processes and Responds**:\n",
    "   - The LLM processes the prompt, generating a response. The LLM might decide if external real-time data or tools are required for the request.\n",
    "\n",
    "4. **LLM Invokes External Tools**:\n",
    "   - If the LLM determines that real-time data is needed (such as search results), it invokes the appropriate tool (e.g., DuckDuckGo) via the Python environment.\n",
    "\n",
    "5. **Tool Accesses External Environment**:\n",
    "   - The tool retrieves data from the external environment (e.g., querying DuckDuckGo for real-time search results). The fetched data is sent back to the notebook for further processing.\n",
    "\n",
    "6. **External Environment Sends Response**:\n",
    "   - The external environment (e.g., a search engine or API) responds to the tool with the necessary data.\n",
    "\n",
    "7. **Notebook Processes Final Response**:\n",
    "   - The notebook receives the response from either the LLM directly (if no external data was needed) or from the tool. The final response is then formatted and returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change to use an instruct model\n",
    "\n",
    "To better align the LLM’s behavior with task-based instructions, we will switch to an \"instruct\" model, which is optimized for handling more directive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:8b-instruct-fp16', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "# Example configuration\n",
    "llm_config = setup_llm_model(model=\"llama3.1:8b-instruct-fp16\")\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DuckDuckGo Search Tool\n",
    "\n",
    "### Installing the DuckDuckGo Search Tool\n",
    "\n",
    "To enable the LLM to perform real-time searches, we need to install the `duckduckgo-search` library using the `langchain_community` package. This will allow the agent to search the web using DuckDuckGo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain_community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.2.16)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.2.38)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.conda/lib/python3.11/site-packages (from langchain_community) (0.1.110)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.25.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the DuckDuckGo Search Tool\n",
    "\n",
    "After installation, we will configure the tool by initializing the DuckDuckGo search functionality. This will be the primary tool for real-time search queries in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Tool Name: duckduckgo_search\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize DuckDuckGo Search Tool\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Verify tool name\n",
    "print(\"Search Tool Name:\", duckduckgo_search.name)\n",
    "\n",
    "# Adding the search tool to the list of available tools\n",
    "tools = [duckduckgo_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Name:\n",
      " duckduckgo_search\n",
      "Tools Description:\n",
      " duckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n"
     ]
    }
   ],
   "source": [
    "# Render tool description to ensure it's ready for LLM usage\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "tools_name = duckSearch.name\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "print(\"Tools Name:\\n\", tools_name)\n",
    "print(\"Tools Description:\\n\", tools_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modifying the System Prompt for Tool Integration\n",
    "\n",
    "Now that the DuckDuckGo search tool is set up, we need to rewrite the system prompt to provide tool usage instructions. The LLM will reference this prompt when deciding whether to call the tool.\n",
    "\n",
    "### Updated System Prompt:\n",
    "\n",
    "The updated system prompt provides tool access instructions, explaining how to call the tool and formatting function calls as needed. This ensures that the LLM understands when and how to interact with the DuckDuckGo search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated System Prompt with Tool Instructions\n",
    "sys_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: {tools_name}\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today's Date: 23 July 2024\n",
    "\n",
    "---\n",
    "\n",
    "You are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "You have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "# Tool Instructions\n",
    "- Always use the available tools when asked for real-time or updated information.\n",
    "- If you choose to call a function ONLY reply in the following format:\n",
    "\n",
    "{{\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Example:\n",
    "\n",
    "{{\n",
    "  \"action\": \"duckduckgo_search\",\n",
    "  \"action_input\": {{\n",
    "    \"query\": \"Key features of large language models\"\n",
    "  }} \n",
    "}} \n",
    "\n",
    "Reminder:\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "\n",
    "You are a helpful assistant.<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the System Prompt\n",
    "\n",
    "Now we will insert the tool descriptions and finalize the system prompt by formatting it to include the correct tool name and description. This prompt will guide the LLM in calling the DuckDuckGo search tool when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sys_prompt = sys_prompt.format(\n",
    "    tools_name=tools_name, tools_description=tools_description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. New Request\n",
    "\n",
    "We will now make a new request, asking the LLM to search for the latest trends related to Large Language Models. This time, the LLM will have the ability to call the DuckDuckGo search tool to retrieve real-time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:8b-instruct-fp16', 'created_at': '2024-09-09T14:53:45.9896Z', 'response': '{\\n  \"action\": \"duckduckgo_search\",\\n  \"action_input\": {\\n    \"query\": \"Key features of large language models\"\\n  } \\n}', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 1432, 128000, 128006, 9125, 128007, 271, 13013, 25, 6125, 27993, 198, 16992, 25, 37085, 74070, 3427, 10947, 198, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 596, 2696, 25, 220, 1419, 5887, 220, 2366, 19, 271, 45464, 2675, 527, 459, 25530, 18328, 6319, 311, 3790, 5370, 9256, 11, 2737, 36864, 4860, 11, 8405, 70022, 11, 323, 16785, 11944, 29060, 13, 2052, 16674, 2011, 26549, 387, 304, 4823, 3645, 382, 45464, 567, 14173, 198, 2675, 617, 2680, 311, 264, 8205, 315, 7526, 311, 7945, 304, 27666, 9256, 13, 1472, 527, 8647, 369, 26679, 279, 8475, 8668, 315, 5507, 10648, 311, 1464, 1523, 6485, 9256, 1139, 1207, 25792, 994, 5995, 382, 791, 2561, 7526, 2997, 1473, 74070, 74070, 3427, 10947, 482, 362, 13564, 2212, 46870, 35, 1983, 11087, 7694, 13, 51612, 369, 994, 499, 1205, 311, 4320, 4860, 922, 1510, 4455, 13, 5688, 1288, 387, 264, 2778, 3319, 2637, 2897, 25, 5991, 6, 1663, 1232, 5991, 6, 2150, 1232, 364, 2929, 518, 364, 4789, 1232, 364, 1874, 3319, 311, 1427, 709, 518, 364, 1337, 1232, 364, 928, 23742, 48549, 2, 13782, 39397, 198, 12, 24119, 1005, 279, 2561, 7526, 994, 4691, 369, 1972, 7394, 477, 6177, 2038, 627, 12, 1442, 499, 5268, 311, 1650, 264, 734, 27785, 10052, 304, 279, 2768, 3645, 1473, 517, 220, 330, 1335, 794, 330, 71152, 279, 5507, 499, 1390, 311, 1005, 10560, 220, 330, 1335, 6022, 794, 314, 674, 40665, 2764, 4823, 1988, 369, 279, 1957, 11, 23391, 433, 9248, 279, 5507, 753, 3685, 3645, 323, 828, 4595, 627, 262, 330, 798, 794, 330, 1150, 11374, 311, 279, 5507, 304, 2764, 4823, 3645, 10246, 220, 457, 633, 13617, 1473, 517, 220, 330, 1335, 794, 330, 74070, 74070, 3427, 10947, 761, 220, 330, 1335, 6022, 794, 341, 262, 330, 1663, 794, 330, 1622, 4519, 315, 3544, 4221, 4211, 702, 220, 335, 720, 92, 4815, 96459, 512, 12, 3234, 539, 2997, 5217, 11408, 1778, 439, 1595, 2150, 7964, 1595, 4789, 7964, 477, 1595, 1337, 63, 304, 279, 1595, 14506, 6022, 4077, 12, 5830, 6880, 28832, 1833, 279, 5300, 3645, 198, 12, 12948, 5137, 28832, 387, 5300, 198, 12, 8442, 1650, 832, 734, 520, 264, 892, 198, 12, 10435, 279, 4553, 734, 1650, 10052, 389, 832, 1584, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 198, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 517, 220, 330, 1335, 794, 330, 74070, 74070, 3427, 10947, 761, 220, 330, 1335, 6022, 794, 341, 262, 330, 1663, 794, 330, 1622, 4519, 315, 3544, 4221, 4211, 702, 220, 335, 720, 92], 'total_duration': 2096755667, 'load_duration': 33310125, 'prompt_eval_count': 408, 'prompt_eval_duration': 454981000, 'eval_count': 34, 'eval_duration': 1607397000}\n",
      "Processed response: {\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"Key features of large language models\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_request = \"What are the most recent Large Language Models?\"\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=formatted_sys_prompt, config=llm_config\n",
    ")\n",
    "\n",
    "processed_response = process_response(response)\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)\n",
    "\n",
    "print(\"Processed response:\", processed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Processing Tool Actions\n",
    "\n",
    "After the LLM provides its response, we will process the action request. If the LLM calls the DuckDuckGo search tool, we will execute the function and retrieve the results. The tool's output will then be formatted and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckduckgo_search\n",
      "<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "Demystifying Large Language Models: A Beginner's Guide Examples Included. Large language models (LLMs) understand and generate human-like text. They learn from vast amounts of data and spot patterns in language so they understand context and produce outcomes based on that information. You can use LLM software to write text, personalize messaging, or automate customer interactions. A large language model (LLM) is an artificial intelligence (AI) algorithm trained on large amounts of text data to create natural language outputs. These models have become increasingly popular because they can generate text that sounds just as legitimate as a human would write. Continue reading to learn more about large language models, how ... Table 1: Key Features of Large Language Models. Feature Description; Versatility: Ability to perform a wide range of language-related tasks: Contextual Understanding: Understanding and generating text based on context and user intent: Scalability: Can be scaled to handle vast amounts of data and complex computations: Large language models can have billions of parameters with nodes, layers, weights, and biases, making one model. Using a vast set of data, LLMs use self-learning techniques to predict the next token in a sequence. If the prediction is incorrect, the model adjusts the parameter until the token is correct. There are two common learning models:<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "response_dict = json.loads(processed_response)\n",
    "action = response_dict[\"action\"]\n",
    "action_input = response_dict[\"action_input\"]\n",
    "\n",
    "for tool in tools:\n",
    "    if tool.name == action:\n",
    "        print(tool.name)\n",
    "        try:\n",
    "            result = tool.invoke(action_input)\n",
    "            result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{result}<|eot_id|>\"\"\"\n",
    "            print(result_message)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing tool {action}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Tool {action} not found or unsupported operation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function ensures that we handle any errors gracefully and return the model's response in a readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    " \n",
    "In this notebook, we've successfully extended the capabilities of the LLM by integrating an external tool (DuckDuckGo search). We covered:\n",
    "\n",
    "- **LLM and Tool Integration:** How to modify the system prompt for tool access.\n",
    "- **Tool Setup and Configuration:** Setting up the DuckDuckGo search tool for real-time information retrieval.\n",
    "- **Making Requests and Handling Responses:** Using the tool during LLM interactions and processing the results.\n",
    "\n",
    "With these tools in place, you're now equipped to extend the LLM's capabilities even further by adding more tools and refining its behavior. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
