{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Understanding ReAct Prompting in Python\n",
    "\n",
    "In this notebook, we will explore the ReAct (Reasoning + Acting) framework, a powerful approach for enhancing the capabilities of large language models (LLMs). ReAct allows models to not only reason through tasks step by step but also to take actions and observe the results, creating a more dynamic problem-solving process.\n",
    "\n",
    "By the end of this tutorial, you'll have a solid understanding of how the ReAct framework works, how it integrates reasoning with actions, and how to set up a simple workflow to implement this approach in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a ReAct Prompting?\n",
    "\n",
    "**ReAct prompting** is a technique that combines reasoning and acting within large language models (LLMs) to solve tasks more effectively. By prompting the model with **task-solving trajectories**, ReAct enables the model to think through a problem step by step while simultaneously taking actions, such as retrieving information or using external tools. This synergy between reasoning and acting allows the model to both plan and execute in a flexible manner.\n",
    "\n",
    "ReAct prompting is particularly effective because it integrates two important processes:\n",
    "- **Reasoning**: The model thinks through the task by generating reasoning traces, helping break down complex queries into manageable steps.\n",
    "- **Acting**: The model performs actions, such as interacting with external tools, APIs, or databases, to gather real-time information that informs its reasoning.\n",
    "\n",
    "For more information, you can read the original paper on ReAct prompting: [ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io/).\n",
    "\n",
    "### How Does ReAct Prompting Work?\n",
    "\n",
    "ReAct prompting follows a **Thought → Action → Observation** loop. Here's how it works:\n",
    "- **Thought**: The model reasons about the task and decides what action to take next.\n",
    "- **Action**: The model executes the chosen action, such as querying an API, calculating a result, or retrieving information.\n",
    "- **Observation**: The model observes the result of the action, updates its internal reasoning, and decides if further actions are needed.\n",
    "\n",
    "This loop continues until the model gathers enough information to provide a complete answer or determines that no further actions are required.\n",
    "\n",
    "### Why Use ReAct Prompting?\n",
    "\n",
    "ReAct prompting allows LLMs to achieve state-of-the-art performance across various tasks by enhancing both reasoning and acting capabilities. It addresses several limitations:\n",
    "- **Misinformation**: In cases where reasoning alone (e.g., chain-of-thought) may lead to errors due to reliance on internal knowledge, ReAct grounding in external actions prevents misinformation.\n",
    "- **Lack of Synthesis**: Acting alone without reasoning can result in incomplete or incoherent solutions. ReAct allows for better synthesis of final answers by combining both reasoning and actions.\n",
    "\n",
    "For example, in the context of our notebook, we'll use ReAct prompting to instruct an agent to use tools like a `basic_calculator` to perform mathematical operations. The agent reasons about when to use the tool, performs the operation, and observes the result to determine if more actions are needed before completing the task.\n",
    "\n",
    "![image.png](images/react-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Before we begin, let's make sure your environment is set up correctly. We'll start by installing the necessary Python packages.\n",
    "\n",
    "### Installing Required Packages\n",
    "\n",
    "To get started, you'll need to install a few Python libraries. Run the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These packages are used for:\n",
    "\n",
    "- **termcolor:** Adding colored text output to the terminal, which is useful for debugging and improving the readability of logs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Function\n",
    "\n",
    "We'll create is a simple function to get the current time. This is important because our agent might need to timestamp certain actions or events. Let's write a function that returns the current date and time in UTC format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current UTC datetime: 2024-09-11 19:41:43.928592 \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def get_current_utc_datetime():\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S.%f UTC\")[:-3]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(\"Current UTC datetime:\", get_current_utc_datetime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring a Simple Model\n",
    "\n",
    "In this section, we configure the machine learning model that we will use to process tasks. The `ModelService` class manages the interaction with the model (in this case, \"llama3.1:8b-instruct-fp16\").\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "We initialize the `ModelService` with a specific model configuration, including parameters such as model endpoint, temperature (for controlling randomness), and others. This step enables us to perform model-based tasks using the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.model_service import ModelService\n",
    "\n",
    "# Initialize the service with the model configuration\n",
    "ollama_service = ModelService(model=\"llama3.1:8b-instruct-fp16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Model\n",
    "\n",
    "The `invoke_model` function handles the communication between the system and the language model. It takes two inputs: `sys_prompt` (system prompt) and `user_prompt` (the user's input), and performs the following steps:\n",
    "\n",
    "1. **Prepare the Payload**: Combines the prompts into a format the model can understand.\n",
    "2. **Send the Request**: Sends the prepared data to the model.\n",
    "3. **Process the Response**: Once the model returns a response, it processes the output into a usable format.\n",
    "\n",
    "The function returns the final processed response, which will be used to guide further actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(sys_prompt: str, user_prompt: str):\n",
    "    \"\"\"\n",
    "    Prepare the payload, send the request to the model, and process the response.\n",
    "    \"\"\"\n",
    "    # Prepare the payload\n",
    "    payload = ollama_service.prepare_payload(\n",
    "        user_prompt,\n",
    "        sys_prompt,\n",
    "    )\n",
    "\n",
    "    # Invoke the model and get the response\n",
    "    response_json = ollama_service.request_model_generate(\n",
    "        payload,\n",
    "    )\n",
    "\n",
    "    # Process the model's response\n",
    "    response_content = ollama_service.process_model_response(response_json)\n",
    "\n",
    "    # Return the processed response\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Tools\n",
    "\n",
    "In this section, we'll show how to integrate custom tools into the workflow. These tools enable the model to perform specific actions when required by the task. We'll begin with a simple calculator tool that can handle basic arithmetic operations.\n",
    "\n",
    "### Basic Calculator Tool\n",
    "\n",
    "The `basic_calculator` tool can perform fundamental operations such as addition, subtraction, multiplication, and division. It takes two numbers and an operation as inputs and returns the result.\n",
    "\n",
    "#### Supported Operations:\n",
    "- `add`: Adds two numbers.\n",
    "- `subtract`: Subtracts one number from another.\n",
    "- `multiply`: Multiplies two numbers.\n",
    "- `divide`: Divides one number by another (raises an error if division by zero).\n",
    "- `modulus`: Returns the remainder of a division.\n",
    "- `power`: Raises one number to the power of another.\n",
    "- Comparison operators: `lt` (less than), `le` (less than or equal to), `eq` (equal), `ne` (not equal), `ge` (greater than or equal to), `gt` (greater than).\n",
    "\n",
    "The model will decide when to use this tool during its reasoning process and provide input in a structured format. Let's take a look at how it's implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def basic_calculator(num1, num2, operation):\n",
    "    \"\"\"\n",
    "    Perform a numeric operation on two numbers based on the input string.\n",
    "\n",
    "    Parameters:\n",
    "    'num1' (int): The first number.\n",
    "    'num2' (int): The second number.\n",
    "    'operation' (str): The operation to perform. Supported operations are 'add', 'subtract',\n",
    "                        'multiply', 'divide', 'floor_divide', 'modulus', 'power', 'lt',\n",
    "                        'le', 'eq', 'ne', 'ge', 'gt'.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted result of the operation.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If an error occurs during the operation (e.g., division by zero).\n",
    "    ValueError: If an unsupported operation is requested or input is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the supported operations\n",
    "    operations = {\n",
    "        \"add\": operator.add,\n",
    "        \"subtract\": operator.sub,\n",
    "        \"multiply\": operator.mul,\n",
    "        \"divide\": operator.truediv,\n",
    "        \"floor_divide\": operator.floordiv,\n",
    "        \"modulus\": operator.mod,\n",
    "        \"power\": operator.pow,\n",
    "        \"lt\": operator.lt,\n",
    "        \"le\": operator.le,\n",
    "        \"eq\": operator.eq,\n",
    "        \"ne\": operator.ne,\n",
    "        \"ge\": operator.ge,\n",
    "        \"gt\": operator.gt,\n",
    "    }\n",
    "\n",
    "    # Check if the operation is supported\n",
    "    if operation in operations:\n",
    "        try:\n",
    "            # Perform the operation\n",
    "            result = operations[operation](num1, num2)\n",
    "            result_formatted = (\n",
    "                f\"The answer is: {result}.\\nCalculated with basic_calculator.\"\n",
    "            )\n",
    "            return result_formatted\n",
    "        except Exception as e:\n",
    "            return str(e), \"\\n\\nError during operation execution.\"\n",
    "    else:\n",
    "        return \"\\n\\nUnsupported operation. Please provide a valid operation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [basic_calculator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool(action: str, action_input: dict):\n",
    "    \"\"\"\n",
    "    Simulate the tool execution based on the action and action_input.\n",
    "    In a real-world scenario, this would call the appropriate tool.\n",
    "    \"\"\"\n",
    "    # Simulate some tool actions (this would be replaced by actual tool logic)\n",
    "    tool_message = f\"\"\"<|python_tag|>{action}.call({action_input})\\n<|eom_id|>\"\"\"\n",
    "    print(\n",
    "        colored(\n",
    "            tool_message,\n",
    "            \"magenta\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for tool in tools:\n",
    "        if tool.name == action:\n",
    "            try:\n",
    "                result = tool.invoke(action_input)\n",
    "                result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{result}<|eot_id|>\"\"\"\n",
    "                print(colored(result_message, \"magenta\"))\n",
    "                return True, result\n",
    "            except Exception as e:\n",
    "                return False, f\"Error executing tool {action}: {str(e)}\"\n",
    "    else:\n",
    "        return f\"Tool {action} not found or unsupported operation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ReAct System Prompt\n",
    "\n",
    "The system prompt provides instructions that guide the model in reasoning through tasks, using tools, and generating structured responses. It defines the context, including the environment (e.g., ipython) and knowledge cut-off date (December 2023), helping the model understand the scope of its information.\n",
    "\n",
    "The model follows a structured process to solve problems by deciding which tools to use and when. Each interaction is formatted in JSON for clear communication.\n",
    "\n",
    "The model operates in a cycle of **thought → action → observation**:\n",
    "- **Thought**: The model reasons about the task and determines what to do next.\n",
    "- **Action**: The model selects and uses the appropriate tool.\n",
    "- **Observation**: The model analyzes the tool's result and decides the next step.\n",
    "\n",
    "This loop continues until the model can provide a final result. If a tool gives a clear answer, the model stops further actions and presents the outcome. If the task cannot be completed, the model will explain the limitation.\n",
    "\n",
    "The system prompt ensures that the model behaves logically, uses tools efficiently, and delivers clear, structured responses.\n",
    "\n",
    "For more information on LLAMA 3.1, refer to the [LLAMA 3.1 Model Card](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n",
    "\n",
    "![React System Prompt](images/react_system_prompt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYS_REACT_PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: {tools_name} \n",
    "Knowledge Cutoff Date: December 2023  \n",
    "Current Date: {datetime}\n",
    "\n",
    "You are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "You have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format:\n",
    "To complete the task, please use the following format:\n",
    "\n",
    "{{\n",
    "  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "After performing an action, the tool will provide a response in the following format:\n",
    "\n",
    "{{\n",
    "  \"observation\": \"The result of the tool invocation\",\n",
    "}}\n",
    "\n",
    "You should keep repeating the format (thought → action → observation) until you have the answer to the original question. \n",
    "\n",
    "If the tool result is successful and the task is complete:\n",
    "\n",
    "{{\n",
    "  \"answer\": \"I have the answer: {{tool_result}}.\"\n",
    "}}\n",
    "\n",
    "\n",
    "Or, if you cannot answer:\n",
    "\n",
    "{{\n",
    "  \"answer\": \"Sorry, I cannot answer your query.\"\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "### Remember:\n",
    "- **If a tool provides a complete and clear answer, do not continue invoking further tools.**\n",
    "- Use the tools effectively and ensure inputs match the required format exactly as described in the task.\n",
    "- Maintain the JSON format and ensure all fields are filled out correctly.\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`.\n",
    "\n",
    "<|eot_id|>\n",
    "{first_user_prompt}\n",
    "{history}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sys_prompt(\n",
    "    first_user_prompt: str = \"\",\n",
    "    history: str = \"\",\n",
    ") -> str:\n",
    "    return DEFAULT_SYS_REACT_PROMPT.format(\n",
    "        first_user_prompt=first_user_prompt,\n",
    "        history=history,\n",
    "        tools_name=get_tools_name(tools),\n",
    "        tools_description=get_tools_description(tools),\n",
    "        datetime=get_current_utc_datetime(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ReAct Loop Implementation\n",
    "\n",
    "The `react` function follows a Thought → Action → Observation loop to process user requests and generate responses using tools.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Prompt Initialization**: The user's request is formatted into a `user_prompt` and a `sys_prompt` to guide the loop.\n",
    "2. **Model Invocation**: The model is called to reason about the task, and the response is parsed.\n",
    "3. **Action Execution**: If an action (e.g., using a tool) is required, the action is performed and the result is observed.\n",
    "4. **History Tracking**: Each step (prompts, responses, and tool outputs) is recorded in a `history` list.\n",
    "5. **Final Output**: The loop continues until a final answer is reached, which is returned along with the history.\n",
    "\n",
    "This structure ensures the model handles tasks iteratively, using tools when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from termcolor import colored\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from utils.general.tools import get_tools_name, get_tools_description\n",
    "\n",
    "# Function to format the scratchpad into a properly indented string\n",
    "def format_history(history):\n",
    "    formatted_output = \"\"\n",
    "    for entry in history:\n",
    "        formatted_output += entry.strip() + \"\\n\"\n",
    "    return formatted_output\n",
    "\n",
    "def react(user_request: str) -> dict:\n",
    "    \"\"\"\n",
    "    Execute the task based on the user's request by following the thought → action → observation loop.\n",
    "    \"\"\"\n",
    "\n",
    "    answer = None\n",
    "\n",
    "    # Start with the user's request as the first input\n",
    "    first_user_prompt = (\n",
    "        f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    "    )\n",
    "\n",
    "    sys_prompt = write_sys_prompt(first_user_prompt=first_user_prompt)\n",
    "\n",
    "    # user_prompt = user_request\n",
    "    tool_response = None\n",
    "    action = None\n",
    "    action_input = None\n",
    "    history = []\n",
    "\n",
    "    print(colored(first_user_prompt, \"green\"))\n",
    "\n",
    "    user_prompt = first_user_prompt\n",
    "\n",
    "    # Loop until a final answer is generated\n",
    "    while answer is None:\n",
    "        # Invoke the model with the system prompt and current user input\n",
    "\n",
    "        response = invoke_model(sys_prompt=sys_prompt, user_prompt=user_prompt)\n",
    "\n",
    "        try:\n",
    "            # Parse the response assuming it's in JSON format\n",
    "            response_dict = json.loads(\n",
    "                response\n",
    "            )  # Assuming response is a JSON object\n",
    "\n",
    "            assistant_message = f\"\"\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\"\"\n",
    "\n",
    "            print(colored(assistant_message, \"cyan\"))\n",
    "\n",
    "            history.append(assistant_message)\n",
    "\n",
    "            formatted_history = format_history(history)\n",
    "            sys_prompt = write_sys_prompt(\n",
    "                first_user_prompt=first_user_prompt, history=formatted_history\n",
    "            )\n",
    "\n",
    "            action = response_dict.get(\"action\", None)\n",
    "            action_input = response_dict.get(\"action_input\", None)\n",
    "\n",
    "            # If there is an action, execute the corresponding tool\n",
    "            if action:\n",
    "                tool_message = (\n",
    "                    f\"\"\"<|python_tag|>{action}.call({action_input})\\n<|eom_id|>\"\"\"\n",
    "                )\n",
    "                history.append(tool_message)\n",
    "                status, tool_response = execute_tool(action, action_input)\n",
    "\n",
    "                # Formulate the observation to feed back into the model\n",
    "                tool_response_dict = {\n",
    "                    \"observation\": tool_response,\n",
    "                }\n",
    "\n",
    "                tool_response_json = json.dumps(tool_response_dict, indent=4)\n",
    "\n",
    "                result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{tool_response_json}<|eot_id|>\"\"\"\n",
    "\n",
    "                print(colored(result_message, \"yellow\"))\n",
    "\n",
    "                user_prompt = tool_response_json\n",
    "                history.append(result_message)\n",
    "\n",
    "            # Check if the model has given an answer\n",
    "            if \"answer\" in response_dict:\n",
    "                answer = response_dict[\"answer\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            system_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{str(e)}<|eot_id|>\"\"\"\n",
    "            history.append(system_message)\n",
    "            formatted_history = format_history(history)\n",
    "            sys_prompt = write_sys_prompt(\n",
    "                first_user_prompt=first_user_prompt, history=formatted_history\n",
    "            )\n",
    "\n",
    "    # Return the final answer\n",
    "    return answer, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the ReAct Workflow\n",
    "\n",
    "Now that we've set up the system, tools, and model, it's time to test the ReAct prompting loop with the `basic_calculator` tool. This will show how the agent reasons through tasks by utilizing thought-action-observation steps.\n",
    "\n",
    "In this test, we will ask the agent to solve a simple arithmetic problem using the calculator tool. Let's see how the ReAct framework works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Please calculate 15 divided by 3, then subtract 2.<|eot_id|>\u001b[0m\n",
      "\u001b[36m<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"thought\": \"To solve this problem, we first need to divide 15 by 3 and store the result.\",\n",
      "    \"action\": \"basic_calculator\",\n",
      "    \"action_input\": {\n",
      "        \"num1\": 15,\n",
      "        \"num2\": 3,\n",
      "        \"operation\": \"divide\"\n",
      "    }\n",
      "}<|eot_id|>\u001b[0m\n",
      "\u001b[35m<|python_tag|>basic_calculator.call({'num1': 15, 'num2': 3, 'operation': 'divide'})\n",
      "<|eom_id|>\u001b[0m\n",
      "\u001b[35m<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "The answer is: 5.0.\n",
      "Calculated with basic_calculator.<|eot_id|>\u001b[0m\n",
      "\u001b[33m<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"observation\": \"The answer is: 5.0.\\nCalculated with basic_calculator.\"\n",
      "}<|eot_id|>\u001b[0m\n",
      "\u001b[36m<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"thought\": \"Now, we need to subtract 2 from the result of the division.\",\n",
      "    \"action\": \"basic_calculator\",\n",
      "    \"action_input\": {\n",
      "        \"num1\": 5,\n",
      "        \"num2\": 2,\n",
      "        \"operation\": \"subtract\"\n",
      "    }\n",
      "}<|eot_id|>\u001b[0m\n",
      "\u001b[35m<|python_tag|>basic_calculator.call({'num1': 5, 'num2': 2, 'operation': 'subtract'})\n",
      "<|eom_id|>\u001b[0m\n",
      "\u001b[35m<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "The answer is: 3.\n",
      "Calculated with basic_calculator.<|eot_id|>\u001b[0m\n",
      "\u001b[33m<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"observation\": \"The answer is: 3.\\nCalculated with basic_calculator.\"\n",
      "}<|eot_id|>\u001b[0m\n",
      "\u001b[36m<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"answer\": \"I have the answer: 3.\"\n",
      "}<|eot_id|>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Please calculate 15 divided by 3, then subtract 2.\"\n",
    "\n",
    "final_answer, history = react(user_request=user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
