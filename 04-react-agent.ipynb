{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Project Manager (PM) Agent for VM Migration in Python\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you will learn how to build and deploy a Project Manager (PM) agent designed to manage the execution of a Virtual Machine (VM) migration plan. This agent will coordinate various tasks, monitor progress, and dynamically adapt the plan based on feedback from different engineering agents. By the end of this notebook, you will understand how to:\n",
    "\n",
    "- Set up and configure a model to support the PM agent's decision-making process.\n",
    "- Define and manage the state for the PM agent to track task progress and dependencies.\n",
    "- Implement and customize the PM agent to handle specific responsibilities within the VM migration workflow.\n",
    "- Construct and compile a workflow graph to represent the PM agent’s interactions with other agents.\n",
    "- Execute the workflow, manage outputs, and handle dynamic adjustments to the plan based on real-time feedback.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "Before diving into the implementation, it's essential to understand some basic concepts:\n",
    "\n",
    "- **PM Agent:** A specialized agent that autonomously manages the execution of a VM migration plan. The PM agent coordinates tasks, assigns responsibilities to other agents (e.g., OCP Engineer, vSphere Engineer), and tracks the overall progress of the migration.\n",
    "\n",
    "- **State:** A shared data structure that stores the context, task statuses, dependencies, and other critical information required by the PM agent. Effective state management ensures that the PM agent can track progress, handle feedback, and adapt the plan as needed.\n",
    "\n",
    "- **VM Migration Plan:** A structured set of tasks required to move virtual machines from one environment to another. The PM agent will break down the migration plan into individual tasks, assign them to the appropriate agents, and ensure they are executed in the correct order.\n",
    "\n",
    "- **Workflow Graph:** A structured representation of the PM agent's workflow, where nodes represent tasks or decisions, and edges define the flow of control between these steps. This graph helps visualize and manage the sequence of actions during the VM migration process.\n",
    "\n",
    "- **Feedback Handling:** The process by which the PM agent receives and processes feedback from other agents (e.g., task completion, errors, or issues) and adapts the migration plan accordingly to ensure a smooth execution.\n",
    "\n",
    "- **Prompt Engineering:** The process of crafting prompts that guide the PM agent's interaction with the model. Proper prompt engineering ensures that the agent generates relevant and accurate tasks and adjustments to the migration plan.\n",
    "\n",
    "Understanding these concepts will provide a solid foundation as we proceed with the practical implementation of the PM agent and its role in managing a VM migration plan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Before we begin, let's make sure your environment is set up correctly. We'll start by installing the necessary Python packages.\n",
    "\n",
    "### Installing Required Packages\n",
    "\n",
    "To get started, you'll need to install a few Python libraries. Run the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: langgraph in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: langgraph-checkpoint-sqlite in ./.conda/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in ./.conda/lib/python3.11/site-packages (from langchain) (0.2.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.conda/lib/python3.11/site-packages (from langchain) (0.1.110)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in ./.conda/lib/python3.11/site-packages (from langgraph) (1.0.8)\n",
      "Requirement already satisfied: aiosqlite<0.21.0,>=0.20.0 in ./.conda/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.8)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./.conda/lib/python3.11/site-packages (from aiosqlite<0.21.0,>=0.20.0->langgraph-checkpoint-sqlite) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langgraph langgraph-checkpoint-sqlite requests termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Blocks\n",
    "\n",
    "In this section, we'll start by building the basic components that our agent will use. These building blocks will form the foundation of our agent, enabling it to keep track of time, store data, and interact with a model.\n",
    "\n",
    "### Datetime Function\n",
    "\n",
    "The first building block we'll create is a simple function to get the current time. This is important because our agent might need to timestamp certain actions or events. Let's write a function that returns the current date and time in UTC format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current UTC datetime: 2024-09-04 20:16:56.436355 \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def get_current_utc_datetime():\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S.%f UTC\")[:-3]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(\"Current UTC datetime:\", get_current_utc_datetime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Agent Data Handling\n",
    "Next, we'll introduce a simple way to handle data for our agent. In this case, we'll create a function that allows us to load and manage the agent's data. This data might include things like the agent's responsibilities, the tasks it needs to perform, and other relevant information.\n",
    "\n",
    "Let's assume the data is stored in a YAML file (a common format for configuration files), and we'll write a function to load this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent descriptions: {'agents': [{'name': 'Planner Agent', 'role': 'Planner Agent', 'responsibilities': ['Creates a comprehensive Migration Plan based on the tutorial.', 'Identifies key steps, target VMs, and source/target providers.', 'Coordinates and structures the plan for execution by other agents.']}, {'name': 'Project Manager (PM) Agent', 'role': 'Project Manager Agent', 'responsibilities': ['Manages the breakdown of tasks for the migration process.', 'Oversees task execution and ensures agents are working in coordination.', 'Ensures timelines are followed and adjusts the plan as necessary.', 'Communicates with all agents to ensure smooth task progression and resolve bottlenecks.']}, {'name': 'vSphere Engineer Agent', 'role': 'vSphere Engineer Agent', 'responsibilities': ['Handles VM identification and configuration within the vSphere environment.', 'Identifies VMs to migrate based on the tutorial instructions.', 'Manages and configures the Migration Toolkit for Virtualization (MTV) to set up the vSphere source providers.', 'Ensures proper VM resource allocation (CPU, memory, storage) within vSphere.', 'Powers off VMs and prepares them for migration if required.', 'Verifies that the VMs in the vSphere environment are ready for migration.']}, {'name': 'OpenShift Engineer Agent', 'role': 'OpenShift Engineer Agent', 'responsibilities': ['Sets up and configures the OpenShift environment for the migrated VMs.', 'Ensures that required OpenShift projects (namespaces) are created and accessible.', 'Configures the Migration Toolkit for Virtualization (MTV) to set up OpenShift as the target provider.', 'Deploys and configures the migrated VMs in OpenShift, ensuring proper integration with storage, networking, and compute resources.', 'Verifies that the OpenShift environment is correctly configured and ready to host migrated VMs.', 'Provides troubleshooting and remediation if issues arise during the setup.', 'Coordinates with the Networking Agent to ensure OpenShift routes, services, and load balancers are configured for migrated applications.']}, {'name': 'Reviewer Agent', 'role': 'Reviewer Agent', 'responsibilities': ['Validates the successful migration of VMs to the target environment.', 'Ensures the application and VMs are functioning correctly post-migration.', 'Checks logs, network settings, and storage allocations for correctness.', 'Provides a final report on the migration’s success.']}]}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def load_agent_descriptions(description_file: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the agent descriptions from a YAML file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(description_file, \"r\") as file:\n",
    "            return yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Description file '{description_file}' not found.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "agents_description = load_agent_descriptions(\"agents.yaml\")\n",
    "print(\"Loaded agent descriptions:\", agents_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuring a Simple Model\n",
    "Now that we have the basic building blocks, we'll move on to configuring a model that our agent can use to perform its tasks. This model will process inputs (like a user request) and generate outputs (like a task list).\n",
    "\n",
    "### Model Configuration\n",
    "We'll start by setting up a simple configuration for the model. This configuration will include details like the model's endpoint, temperature, and other parameters. Let's create a function to handle this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3:instruct', 'temperature': 0.0, 'top_p': 1.0, 'top_k': 0, 'repetition_penalty': 1.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_ollama_model(\n",
    "    model, temperature=0.0, top_p=1.0, top_k=0, repetition_penalty=1.0, stop=None\n",
    "):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration:\n",
    "ollama_config = setup_ollama_model(model=\"llama3:instruct\")\n",
    "print(\"Model configuration:\", ollama_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a dictionary with the model's configuration. You can adjust the parameters based on the specific model you're using or the task requirements.\n",
    "\n",
    "### Preparing a Request\n",
    "With the model configured, the next step is to prepare a request that the agent can send to the model. This request will include the user's input, the system's instructions, and any other necessary information. Let's write a function to prepare this request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_payload(\n",
    "    user_prompt: str,\n",
    "    sys_prompt: str,\n",
    "    stream: bool = False,\n",
    "    config: Dict[str, Any] = ollama_config,\n",
    ") -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"model\": config.get(\"model\"),\n",
    "        \"format\": \"json\",\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"stream\": stream,\n",
    "        \"temperature\": config.get(\"temperature\", 0.0),\n",
    "        \"top_p\": config.get(\"top_p\", 1.0),\n",
    "        \"top_k\": config.get(\"top_k\", 0),\n",
    "        \"repetition_penalty\": config.get(\"repetition_penalty\", 1.0),\n",
    "        \"stop\": config.get(\"stop\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending a Request\n",
    "We'll start by writing a function to send the request to the model's endpoint and receive a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def request_model_generate_endpoint(\n",
    "    payload: Dict[str, Any], config: Dict[str, Any] = ollama_config\n",
    ") -> Dict[str, Any]:\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config.get(\"model_endpoint\"),\n",
    "            headers=config.get(\"headers\", {\"Content-Type\": \"application/json\"}),\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if response.content.strip():\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": \"Empty response from model\"}\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prepared payload to the model's endpoint using the `requests` library. It then checks if the response is valid and returns the content. If there's an error in the request, it raises an exception with a descriptive message.\n",
    "\n",
    "### Processing the Response\n",
    "Finally, we'll write a function to process and understand the model's response. This might involve formatting the response or extracting specific information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_response(response_json: Dict[str, Any]) -> str:\n",
    "    try:\n",
    "        response_content = json.loads(response_json.get(\"response\", \"{}\"))\n",
    "        pretty_content = json.dumps(response_content, indent=4)\n",
    "\n",
    "        return pretty_content\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Error processing the response\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def basic_calculator(num1, num2, operation):\n",
    "    \"\"\"\n",
    "    Perform a numeric operation on two numbers based on the input string.\n",
    "\n",
    "    Parameters:\n",
    "    'num1' (int): The first number.\n",
    "    'num2' (int): The second number.\n",
    "    'operation' (str): The operation to perform. Supported operations are 'add', 'subtract',\n",
    "                        'multiply', 'divide', 'floor_divide', 'modulus', 'power', 'lt',\n",
    "                        'le', 'eq', 'ne', 'ge', 'gt'.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted result of the operation.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If an error occurs during the operation (e.g., division by zero).\n",
    "    ValueError: If an unsupported operation is requested or input is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the supported operations\n",
    "    operations = {\n",
    "        \"add\": operator.add,\n",
    "        \"subtract\": operator.sub,\n",
    "        \"multiply\": operator.mul,\n",
    "        \"divide\": operator.truediv,\n",
    "        \"floor_divide\": operator.floordiv,\n",
    "        \"modulus\": operator.mod,\n",
    "        \"power\": operator.pow,\n",
    "        \"lt\": operator.lt,\n",
    "        \"le\": operator.le,\n",
    "        \"eq\": operator.eq,\n",
    "        \"ne\": operator.ne,\n",
    "        \"ge\": operator.ge,\n",
    "        \"gt\": operator.gt,\n",
    "    }\n",
    "\n",
    "    # Check if the operation is supported\n",
    "    if operation in operations:\n",
    "        try:\n",
    "            # Perform the operation\n",
    "            result = operations[operation](num1, num2)\n",
    "            result_formatted = (\n",
    "                f\"\\n\\nThe answer is: {result}.\\nCalculated with basic_calculator.\"\n",
    "            )\n",
    "            return result_formatted\n",
    "        except Exception as e:\n",
    "            return str(e), \"\\n\\nError during operation execution.\"\n",
    "    else:\n",
    "        return \"\\n\\nUnsupported operation. Please provide a valid operation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "# To use these tools within our agent, we register them in a list. This list will be referenced by the agent to determine which tools are available for use.\n",
    "tools = [basic_calculator]\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. State Management\n",
    "\n",
    "State management is an essential concept when working with agents. The \"state\" of an agent refers to its current condition or the information it has at any given time. For instance, if an agent is working through a list of tasks, its state might include which tasks have been completed, which are in progress, and which are yet to be started.\n",
    "\n",
    "Managing this state is crucial because it allows the agent to keep track of what it has done and what it needs to do next. Without proper state management, an agent might lose track of its progress, repeat tasks, or skip important steps. In this notebook, you'll learn how to manage an agent's state effectively, ensuring that it operates smoothly and efficiently.\n",
    "\n",
    "### Implementation of State Management\n",
    "\n",
    "To implement state management for our PM agent, we define a structured data model (`AgentGraphState`) that holds all the relevant information the agent needs to function effectively. This model includes:\n",
    "- **Input:** The current command or task that the agent is working on.\n",
    "- **Response:** The outputs or actions the agent has generated.\n",
    "- **Feedback:** Any feedback received from other agents or parts of the system.\n",
    "- **Validation Status:** A flag indicating whether the agent's output has been validated, ensuring that all tasks meet the required criteria before proceeding.\n",
    "\n",
    "Additionally, we provide a utility function, `update_state`, which allows for updating specific elements of the agent's state. This function ensures that the state is consistently and accurately maintained, which is critical for the agent to operate effectively. By checking for the existence of keys before updating, the function helps prevent errors and maintains the integrity of the state.\n",
    "\n",
    "Together, these components form the backbone of the agent's state management system, enabling it to manage complex workflows and adapt to changes dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated, TypedDict, Any\n",
    "\n",
    "class AgentGraphState(TypedDict):\n",
    "    input: str\n",
    "    response: Annotated[list, add_messages]\n",
    "    feedback: Annotated[list, add_messages]\n",
    "    validated: bool = False\n",
    "\n",
    "\n",
    "def update_state(state: AgentGraphState, key: str, value: Any):\n",
    "    \"\"\"\n",
    "    Update the state of the agent. Warn if the key doesn't exist.\n",
    "    \"\"\"\n",
    "    if key in state:\n",
    "        state[key] = value\n",
    "    else:\n",
    "        print(f\"Warning: Attempting to update a non-existing state key '{key}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What is an Agent?\n",
    "\n",
    "In simple terms, an agent is a program that can perform tasks autonomously based on a set of instructions. Think of an agent as a virtual assistant that can handle specific jobs for you. For example, in the context of data science or software development, an agent might process data, make decisions based on that data, and then carry out actions like sending requests or updating records.\n",
    "\n",
    "Agents are often designed to work without constant human intervention. Once you give them the initial instructions, they can execute tasks on their own, making them very useful in automating repetitive or complex processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### System Prompts\n",
    "\n",
    "System prompts are predefined instructions provided to large language models (LLMs) that guide the AI's behavior during interactions. They set the context, define the role of the AI, and establish rules for how the model should respond to user inputs.\n",
    "\n",
    "#### Importance of System Prompts\n",
    "\n",
    "System prompts are essential for ensuring that an AI model like our PM agent performs its tasks consistently and effectively. By specifying the role, task guidelines, and response format, the system prompt helps the AI maintain focus on its objectives—in this case, managing a VM migration plan. \n",
    "\n",
    "#### How They Work\n",
    "\n",
    "When the PM agent receives a query, the system prompt shapes how it processes the input and generates its output. This includes breaking down tasks, assigning them to the correct agents, and handling feedback—all within the structure provided by the prompt.\n",
    "\n",
    "In summary, system prompts are a powerful tool that directs AI behavior, ensuring responses are aligned with the desired goals and context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYS_REACT_PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython  \n",
    "Knowledge Cutoff Date: December 2023  \n",
    "Current Date: {datetime}\n",
    "\n",
    "You are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "You have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format:\n",
    "To complete the task, please use the following format:\n",
    "\n",
    "{{\n",
    "  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "After performing an action, the tool will provide a response in the following format:\n",
    "\n",
    "{{\n",
    "  \"observation\": \"The result of the tool invocation\",\n",
    "}}\n",
    "\n",
    "You should keep repeating the format (thought → action → observation) until you have gathered enough information to answer the question. **If the observation provides a clear and complete answer to the user's query, immediately conclude with the final answer and do not perform further actions.** Once you have sufficient information, respond using one of the following formats:\n",
    "\n",
    "\n",
    "If the tool result is successful and the task is complete:\n",
    "\n",
    "{{\n",
    "  \"thought\": \"The tool '{{action}}' executed successfully, and the output meets the acceptance criteria. No further actions are required.\",\n",
    "  \"final_answer\": \"The task has been completed successfully with the tool output: {{tool_result}}.\"\n",
    "}}\n",
    "\n",
    "Or, if you cannot answer:\n",
    "\n",
    "{{\n",
    "  \"thought\": \"The tool '{{action}}' failed to execute successfully. The error was: {{tool_result}}. Here is what went wrong and what needs to be corrected: [Provide corrections or adjustments].\",\n",
    "  \"action_correction\": \"Description of what needs to be adjusted or corrected before retrying.\"\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "### Remember:\n",
    "- Use the tools effectively and ensure inputs match the required format exactly as described in the task.\n",
    "- **If a tool provides a complete and clear answer, do not continue invoking further tools.**\n",
    "- Maintain the JSON format and ensure all fields are filled out correctly.\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`.\n",
    "\n",
    "---\n",
    "\n",
    "## Current Conversation\n",
    "Below is the ongoing conversation consisting of interleaving human and assistant messages:\n",
    "\n",
    "{agent_scratchpad}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_react_prompt(\n",
    "    agent_scratchpad: str = \"\",\n",
    "    tools_description: str = tools_description,\n",
    ") -> str:\n",
    "    return DEFAULT_SYS_REACT_PROMPT.format(\n",
    "        agent_scratchpad=agent_scratchpad,\n",
    "        tools_description=tools_description,\n",
    "        datetime=get_current_utc_datetime(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3:instruct', 'format': 'json', 'prompt': 'What tasks should I complete in order to make pasta?', 'system': '\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nEnvironment: ipython  \\nKnowledge Cutoff Date: December 2023  \\nCurrent Date: 2024-09-04 20:16:56.538407 \\n\\nYou are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\\n\\n---\\n\\n## Tools\\nYou have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\\n\\nThe available tools include:\\n\\nbasic_calculator(num1, num2, operation) - Perform a numeric operation on two numbers based on the input string. Parameters:\\n\\'num1\\' (int): The first number.\\n\\'num2\\' (int): The second number.\\n\\'operation\\' (str): The operation to perform. Supported operations are \\'add\\', \\'subtract\\',\\n                    \\'multiply\\', \\'divide\\', \\'floor_divide\\', \\'modulus\\', \\'power\\', \\'lt\\',\\n                    \\'le\\', \\'eq\\', \\'ne\\', \\'ge\\', \\'gt\\'., args: {{\\'num1\\': {{\\'title\\': \\'Num1\\'}}, \\'num2\\': {{\\'title\\': \\'Num2\\'}}, \\'operation\\': {{\\'title\\': \\'Operation\\'}}}}\\n\\n---\\n\\n## Output Format:\\nTo complete the task, please use the following format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": { # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\\n    \"key\": \"Value inputs to the tool in valid JSON format.\"\\n  }\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\",\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have gathered enough information to answer the question. **If the observation provides a clear and complete answer to the user\\'s query, immediately conclude with the final answer and do not perform further actions.** Once you have sufficient information, respond using one of the following formats:\\n\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"thought\": \"The tool \\'{action}\\' executed successfully, and the output meets the acceptance criteria. No further actions are required.\",\\n  \"final_answer\": \"The task has been completed successfully with the tool output: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"thought\": \"The tool \\'{action}\\' failed to execute successfully. The error was: {tool_result}. Here is what went wrong and what needs to be corrected: [Provide corrections or adjustments].\",\\n  \"action_correction\": \"Description of what needs to be adjusted or corrected before retrying.\"\\n}\\n\\n---\\n\\n### Remember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described in the task.\\n- **If a tool provides a complete and clear answer, do not continue invoking further tools.**\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`.\\n\\n---\\n\\n## Current Conversation\\nBelow is the ongoing conversation consisting of interleaving human and assistant messages:\\n\\n\\n<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n', 'stream': False, 'temperature': 0.0, 'top_p': 1.0, 'top_k': 0, 'repetition_penalty': 1.0, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "payload = prepare_payload(\n",
    "    user_prompt=\"What tasks should I complete in order to make pasta?\",\n",
    "    sys_prompt=write_react_prompt(),\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Implementing the Agent\n",
    "\n",
    "### Creating a Simple Agent Class\n",
    "\n",
    "In this step, we define the `Agent` class, which is responsible for managing the state of the PM agent, interacting with the model, and processing the responses. This class encapsulates the core functionalities needed to execute tasks autonomously.\n",
    "\n",
    "### Key Components:\n",
    "- **Initialization (`__init__`)**: The constructor initializes the agent with its state, role, and model configuration. This setup is crucial for ensuring that the agent operates within the defined parameters and context.\n",
    "  \n",
    "- **Model Invocation (`invoke_model`)**: This method prepares the input payload, sends it to the model for processing, and handles the model's response. It's where the agent interacts with the LLM, using the system prompt and user prompt to generate meaningful outputs.\n",
    "\n",
    "- **Task Execution (`execute_task`)**: This method allows the agent to execute a specific task based on a user request. It utilizes the system prompt tailored for the task and processes the response generated by the model.\n",
    "\n",
    "The `Agent` class is fundamental in making our PM agent autonomous, enabling it to perform its duties without constant human oversight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state: AgentGraphState, role: str, model_config: dict):\n",
    "        \"\"\"\n",
    "        Initialize the Agent with a state, role, and model configuration.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.role = role\n",
    "        self.model_config = model_config\n",
    "\n",
    "    def invoke_model(self, sys_prompt: str, user_prompt: str):\n",
    "        \"\"\"\n",
    "        Prepare the payload, send the request to the model, and process the response.\n",
    "        \"\"\"\n",
    "        # Prepare the payload\n",
    "        payload = prepare_payload(user_prompt, sys_prompt, config=self.model_config)\n",
    "\n",
    "        # Invoke the model and get the response\n",
    "        response_json = request_model_generate_endpoint(\n",
    "            payload, config=self.model_config\n",
    "        )\n",
    "\n",
    "        # Process the model's response\n",
    "        response_content = process_model_response(response_json)\n",
    "\n",
    "        # Return the processed response\n",
    "        return response_content\n",
    "\n",
    "    def react(self, user_request: str) -> dict:\n",
    "        \"\"\"\n",
    "        Execute the task based on the user's request by following the thought → action → observation loop.\n",
    "        \"\"\"\n",
    "        sys_prompt = write_react_prompt()\n",
    "        final_answer = None\n",
    "\n",
    "        # Start with the user's request as the first input\n",
    "        user_prompt = user_request\n",
    "        action = None\n",
    "        action_input = None\n",
    "        scratchpad = []\n",
    "\n",
    "        human_message = HumanMessage(content=user_prompt)\n",
    "        print(colored(human_message.pretty_repr(), \"green\"))\n",
    "\n",
    "        # Loop until a final answer is generated\n",
    "        while final_answer is None:\n",
    "            # Invoke the model with the system prompt and current user input\n",
    "\n",
    "            response = self.invoke_model(sys_prompt=sys_prompt, user_prompt=user_prompt)\n",
    "\n",
    "            try:\n",
    "                # Parse the response assuming it's in JSON format\n",
    "                response_dict = json.loads(response)  # Assuming response is a JSON object\n",
    "\n",
    "                ai_message = AIMessage(content=response)\n",
    "                print(colored(ai_message.pretty_repr(), \"cyan\"))\n",
    "\n",
    "                scratchpad.append(ai_message)\n",
    "\n",
    "                action = response_dict.get(\"action\", None)\n",
    "                action_input = response_dict.get(\"action_input\", None)\n",
    "\n",
    "                # If there is an action, execute the corresponding tool\n",
    "                if action and action_input:\n",
    "                    status, tool_response = self.execute_tool(action, action_input)\n",
    "\n",
    "                    # Formulate the observation to feed back into the model\n",
    "                    tool_response_dict = {\n",
    "                        \"observation\": tool_response,\n",
    "                    }\n",
    "\n",
    "                    tool_response_json = json.dumps(tool_response_dict, indent=4)\n",
    "\n",
    "                    tool_system_message = SystemMessage(content=tool_response_json)\n",
    "                    print(colored(tool_system_message.pretty_repr(), \"yellow\"))\n",
    "\n",
    "                    user_prompt = tool_response_json\n",
    "\n",
    "                # Check if the model has given a final answer\n",
    "                if \"final_answer\" in response_dict:\n",
    "                    final_answer = response_dict[\"final_answer\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                error_message = SystemMessage(content=str(e))\n",
    "                scratchpad.append(error_message)\n",
    "\n",
    "        # Return the final answer\n",
    "        return {\"response\": AIMessage(content=final_answer)}\n",
    "\n",
    "    def execute_tool(self, action: str, action_input: dict):\n",
    "        \"\"\"\n",
    "        Simulate the tool execution based on the action and action_input.\n",
    "        In a real-world scenario, this would call the appropriate tool.\n",
    "        \"\"\"\n",
    "        # Simulate some tool actions (this would be replaced by actual tool logic)\n",
    "        print(\n",
    "            colored(\n",
    "                \"================================ Calling Tool ================================\",\n",
    "                \"magenta\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(colored(f\"Tool: {action}\", \"magenta\"))\n",
    "        print(colored(f\"Tool Input: {action_input}\", \"magenta\"))\n",
    "\n",
    "        for tool in tools:\n",
    "            if tool.name == action:\n",
    "                try:\n",
    "                    result = tool.invoke(action_input)\n",
    "                    print(colored(f\"Tool Result: {result}\", \"magenta\"))\n",
    "                    return True, result\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error executing tool {action}: {str(e)}\"\n",
    "        else:\n",
    "            return f\"Tool {action} not found or unsupported operation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Creating and Compiling the Workflow Graph\n",
    "\n",
    "### Constructing the PM Agent's Workflow\n",
    "\n",
    "In this step, we build the workflow graph that represents the PM agent's process. This graph outlines the sequence of operations, including task execution, validation, and decision-making.\n",
    "\n",
    "### Key Components:\n",
    "- **Node Definitions**: Each node in the graph represents a step in the workflow, such as invoking the PM agent or validating the output.\n",
    "  \n",
    "- **Edge Definitions**: Edges define the flow between nodes, determining how the agent progresses through the tasks and validation steps.\n",
    "\n",
    "- **Workflow Compilation**: Once the graph is defined, it is compiled into a workflow that can be executed. This compiled workflow represents the full sequence of operations that the PM agent will follow to manage the VM migration.\n",
    "\n",
    "By constructing and compiling this workflow graph, we ensure that the PM agent operates in a structured and efficient manner, handling tasks and making decisions in a logical sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_node_function(state: AgentGraphState):\n",
    "    react_agent = Agent(\n",
    "        state=state,\n",
    "        role=\"REACT_AGENT\",\n",
    "        model_config=ollama_config,\n",
    "    )\n",
    "\n",
    "    return react_agent.react(user_request=state[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "def create_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Create the state graph by defining nodes and edges.\n",
    "\n",
    "    Returns:\n",
    "    - StateGraph: The compiled state graph ready for execution.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentGraphState)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"react_agent\", react_node_function)\n",
    "\n",
    "    # Define the flow of the graph\n",
    "    graph.add_edge(START, \"react_agent\")\n",
    "    graph.add_edge(\"react_agent\", END)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Sqlite Persistence for Graph State\n",
    "\n",
    "In this cell, we define and use a method to initialize an `SqliteSaver` instance from the `langgraph.checkpoint.sqlite` module. The `SqliteSaver` class allows the graph state to be persisted in an SQLite database, which is more durable and suitable for applications requiring longer-term storage compared to an in-memory solution.\n",
    "\n",
    "The `from_conn_stringx` method is defined as a class method that takes a connection string as input, creates a connection to the SQLite database using `sqlite3.connect`, and then returns an `SqliteSaver` instance using this connection. This method simplifies the creation of an `SqliteSaver` instance directly from a connection string.\n",
    "\n",
    "This approach is particularly useful for ensuring that the state of the `StateGraph` is saved to a local or memory-based SQLite database, enabling the retention of context across multiple interactions in AI-driven applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "def from_conn_stringx(\n",
    "    cls,\n",
    "    conn_string: str,\n",
    ") -> \"SqliteSaver\":\n",
    "    return SqliteSaver(conn=sqlite3.connect(conn_string, check_same_thread=False))\n",
    "\n",
    "\n",
    "SqliteSaver.from_conn_stringx = classmethod(from_conn_stringx)\n",
    "\n",
    "memory = SqliteSaver.from_conn_stringx(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Executing the Workflow\n",
    "\n",
    "With the workflow graph compiled, the final step is to execute the workflow. This involves providing the agent with an input query, such as a request to generate a VM migration plan, and allowing the workflow to run through its defined sequence.\n",
    "\n",
    "### Creating and Running the Workflow\n",
    "\n",
    "In this step, we create and execute the PM agent's workflow to process a set of tasks.\n",
    "\n",
    "- **Graph Creation**: \n",
    "  - We first create the workflow graph using `create_graph()` and compile it with a memory-based checkpoint.\n",
    "  - The compiled workflow will manage the task execution, validation, and feedback handling.\n",
    "\n",
    "- **Workflow Parameters**:\n",
    "  - We define the number of iterations (`iterations = 10`), set verbose mode to `True`, and configure the thread ID.\n",
    "  - A query containing three tasks (VM details retrieval, migration plan creation, and migration start) is provided as input.\n",
    "\n",
    "- **Workflow Execution**:\n",
    "  - The workflow is executed using `workflow.stream()`, and it processes each task sequentially.\n",
    "  - Depending on the state of the workflow, feedback or task responses are printed to track progress.\n",
    "\n",
    "This step runs the agent through the defined tasks and prints the state changes for each event in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph and workflow created.\n",
      "\u001b[32m================================ Human Message =================================\n",
      "\n",
      "What is 10+10?\u001b[0m\n",
      "\u001b[36m================================== Ai Message ==================================\n",
      "\n",
      "{\n",
      "    \"thought\": \"To determine the result, I will use the basic calculator tool to add two numbers.\",\n",
      "    \"action\": \"basic_calculator\",\n",
      "    \"action_input\": {\n",
      "        \"num1\": 10,\n",
      "        \"num2\": 10,\n",
      "        \"operation\": \"add\"\n",
      "    }\n",
      "}\u001b[0m\n",
      "\u001b[35m================================ Calling Tool ================================\u001b[0m\n",
      "\u001b[35mTool: basic_calculator\u001b[0m\n",
      "\u001b[35mTool Input: {'num1': 10, 'num2': 10, 'operation': 'add'}\u001b[0m\n",
      "\u001b[35mTool Result: \n",
      "\n",
      "The answer is: 20.\n",
      "Calculated with basic_calculator.\u001b[0m\n",
      "\u001b[33m================================ System Message ================================\n",
      "\n",
      "{\n",
      "    \"observation\": \"\\n\\nThe answer is: 20.\\nCalculated with basic_calculator.\"\n",
      "}\u001b[0m\n",
      "\u001b[36m================================== Ai Message ==================================\n",
      "\n",
      "{\n",
      "    \"thought\": \"It seems like we've already calculated the result, so there's no need to perform further actions. The final answer has been determined.\",\n",
      "    \"final_answer\": \"The task has been completed successfully with the tool output: 20.\\nCalculated with basic_calculator.\"\n",
      "}\u001b[0m\n",
      "\n",
      "Event: {'react_agent': {'response': AIMessage(content='The task has been completed successfully with the tool output: 20.\\nCalculated with basic_calculator.')}}\n"
     ]
    }
   ],
   "source": [
    "# Create the graph and compile the workflow\n",
    "graph = create_graph()\n",
    "workflow = graph.compile(checkpointer=memory)\n",
    "print(\"Graph and workflow created.\")\n",
    "\n",
    "# Define workflow parameters\n",
    "iterations = 10\n",
    "verbose = True\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "query = \"What is 10+10?\"\n",
    "dict_inputs = {\"input\": query}\n",
    "limit = {\"recursion_limit\": iterations}\n",
    "\n",
    "# Execute the workflow and print state changes\n",
    "for event in workflow.stream(dict_inputs, config):\n",
    "    if verbose:\n",
    "            print(\"\\nEvent:\", event)\n",
    "    else:\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
