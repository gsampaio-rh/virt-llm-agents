{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Large Language Models (LLMs), Prompts\n",
    "Welcome to this tutorial on building a simple LLM-powered agent in Python! In this notebook, we will explore the fundamentals of Large Language Models (LLMs) and prompt engineering. The focus will be on understanding how LLMs work, what prompts are, and how to effectively communicate with LLMs to achieve the best possible responses.\n",
    "\n",
    "By the end of this tutorial, you'll understand how to configure a simple LLM, craft effective prompts, and make your first call to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Prompts\n",
    "\n",
    "### What is a Large Language Model (LLM)?\n",
    "\n",
    "A Large Language Model (LLM) is an advanced AI model designed to understand and generate human-like text. LLMs are trained on vast amounts of data and can be used for tasks like answering questions, summarizing information, generating content, and more.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to guide its response. It consists of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the context, role, or instructions for the model.\n",
    "- **User Prompt:** This is the specific request or question the user wants the LLM to address.\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response. Let's break down some key terms used in working with LLMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "![image](images/llm_flow.png)\n",
    "\n",
    "1. **Define Request**: The user defines their request to the LLM.\n",
    "2. **Payload Preparation**\n",
    "\t* **System Prompt**: Sets the role or behavior of the LLM.\n",
    "\t* **User Prompt**: Specifies the question or task to be solved.\n",
    "\t* **Parameters**: Additional settings for fine-tuning the response (e.g., temperature, stop sequences).\n",
    "3. **Send Request to LLM**: The prepared payload is sent to the LLM for processing.\n",
    "4. **LLM Internal Processing**\n",
    "\t* **Tokenization**: Breaks down input into smaller parts (tokens).\n",
    "\t* **Inference/Computation**: Computes a response based on input tokens.\n",
    "\t* **Detokenization**: Converts output tokens back to human-readable text.\n",
    "\t* **Post-Processing**: Makes final adjustments based on parameters.\n",
    "5. **Receive & Process Response**: The LLM generates and sends the response.\n",
    "6. **Output Result**: The processed response is displayed to the user.\n",
    "7. **End**: The process concludes once the result is delivered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `requests` library to send our prompts to the LLM endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring the LLM\n",
    "\n",
    "Next, we’ll set up a configuration to define which model to use and other parameters like temperature and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration: {'model_endpoint': 'http://localhost:11434/api/generate', 'model': 'llama3.1:latest', 'temperature': 0.0, 'headers': {'Content-Type': 'application/json'}, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model(model=\"llama3.1:latest\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model_endpoint\": \"http://localhost:11434/api/generate\",\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "llm_config = setup_llm_model()\n",
    "print(\"Model configuration:\", llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the configuration for the LLM, including the endpoint and parameters like temperature and stop sequence.\n",
    "\n",
    "### Key Parameters in LLM Interaction\n",
    "\n",
    "- **Temperature:** This parameter controls the randomness or creativity of the model's responses. A **higher temperature** (e.g., 0.8 or above) results in more diverse and creative outputs, while a **lower temperature** (e.g., 0.2) generates more predictable and deterministic responses. For instance:\n",
    "  - Low temperature (0.2): More focused and concise answers.\n",
    "  - High temperature (0.8): Responses that may include more creative and less common interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crafting the Prompts\n",
    " \n",
    "In the context of Large Language Models (LLMs), **prompts** are crucial for guiding the model's behavior and obtaining useful outputs. They consist of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system and user prompts\n",
    "sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>\"\n",
    "\n",
    "user_request = \"What are the key features of Large Language Models?\"\n",
    "user_prompt = (\n",
    "    f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared payload: {'model': 'llama3.1:latest', 'prompt': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key features of Large Language Models?<|eot_id|>', 'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant that provides concise and accurate answers.<|eot_id|>', 'temperature': 0.0, 'stop': None, 'stream': False}\n"
     ]
    }
   ],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, config: dict):\n",
    "    return {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"stop\": config[\"stop\"],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(\n",
    "    user_prompt=user_prompt, sys_prompt=sys_prompt, config=llm_config\n",
    ")\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload contains the system and user prompts along with the LLM configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making a Call to the LLM\n",
    "\n",
    "Now that we have the prompts prepared, we’ll send the request to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: {'model': 'llama3.1:latest', 'created_at': '2024-09-11T17:43:11.467747Z', 'response': 'Large Language Models (LLMs) have several key features that enable them to process and generate human-like language:\\n\\n1. **Self-Supervised Learning**: LLMs learn from vast amounts of text data without explicit supervision, using techniques like masked language modeling or next sentence prediction.\\n2. **Transformer Architecture**: Most popular LLMs are based on the Transformer architecture, which allows for parallel processing of input sequences and enables efficient computation.\\n3. **Multilayer Perceptron (MLP) Layers**: LLMs typically consist of a series of MLP layers with self-attention mechanisms that weigh inputs according to their relevance.\\n4. **Self-Attention Mechanisms**: These allow the model to focus on specific parts of the input sequence when making predictions, emulating human language processing.\\n5. **Pretraining and Fine-tuning**: LLMs are pre-trained on large datasets and can then be fine-tuned for a specific task or domain.\\n6. **Scalability**: LLMs can process sequences of varying lengths, from short text fragments to long documents.\\n7. **Contextual Understanding**: They capture contextual relationships between words, phrases, and sentences, enabling the model to understand nuances and subtleties in language.\\n8. **Generative Capabilities**: Many LLMs have generative capabilities, allowing them to produce coherent and contextually relevant text.\\n\\nSome popular examples of Large Language Models include:\\n\\n1. BERT (Bidirectional Encoder Representations from Transformers)\\n2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\\n3. XLNet\\n4. Longformer (Long-range Transformers for Automatic Speech Recognition)\\n5. T5 (Text-to-Text Transfer Transformer)\\n\\nThese models have achieved state-of-the-art performance in various natural language processing tasks, such as text classification, question answering, sentiment analysis, and machine translation.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 128000, 128006, 9125, 128007, 2675, 527, 264, 11190, 18328, 430, 5825, 64694, 323, 13687, 11503, 13, 128009, 128009, 128006, 882, 128007, 271, 128006, 882, 128007, 271, 3923, 527, 279, 1401, 4519, 315, 20902, 11688, 27972, 30, 128009, 128009, 128006, 78191, 128007, 271, 35353, 11688, 27972, 320, 4178, 22365, 8, 617, 3892, 1401, 4519, 430, 7431, 1124, 311, 1920, 323, 7068, 3823, 12970, 4221, 1473, 16, 13, 3146, 12363, 6354, 455, 79090, 21579, 96618, 445, 11237, 82, 4048, 505, 13057, 15055, 315, 1495, 828, 2085, 11720, 38217, 11, 1701, 12823, 1093, 43248, 4221, 34579, 477, 1828, 11914, 20212, 627, 17, 13, 3146, 47458, 38943, 96618, 7648, 5526, 445, 11237, 82, 527, 3196, 389, 279, 63479, 18112, 11, 902, 6276, 369, 15638, 8863, 315, 1988, 24630, 323, 20682, 11297, 35547, 627, 18, 13, 3146, 41504, 321, 1155, 3700, 346, 95810, 320, 2735, 47, 8, 84922, 96618, 445, 11237, 82, 11383, 6824, 315, 264, 4101, 315, 95991, 13931, 449, 659, 12, 54203, 24717, 430, 17988, 11374, 4184, 311, 872, 41961, 627, 19, 13, 3146, 12363, 12, 70429, 28901, 13978, 96618, 4314, 2187, 279, 1646, 311, 5357, 389, 3230, 5596, 315, 279, 1988, 8668, 994, 3339, 20492, 11, 991, 15853, 3823, 4221, 8863, 627, 20, 13, 3146, 4808, 31754, 323, 31253, 2442, 38302, 96618, 445, 11237, 82, 527, 864, 70024, 389, 3544, 30525, 323, 649, 1243, 387, 7060, 2442, 49983, 369, 264, 3230, 3465, 477, 8106, 627, 21, 13, 3146, 3407, 278, 2968, 96618, 445, 11237, 82, 649, 1920, 24630, 315, 29865, 29416, 11, 505, 2875, 1495, 35603, 311, 1317, 9477, 627, 22, 13, 3146, 2014, 940, 46551, 96618, 2435, 12602, 66251, 12135, 1990, 4339, 11, 32847, 11, 323, 23719, 11, 28462, 279, 1646, 311, 3619, 84889, 323, 42129, 1169, 552, 304, 4221, 627, 23, 13, 3146, 5648, 1413, 8171, 8623, 96618, 9176, 445, 11237, 82, 617, 1803, 1413, 17357, 11, 10923, 1124, 311, 8356, 56887, 323, 2317, 1870, 9959, 1495, 382, 8538, 5526, 10507, 315, 20902, 11688, 27972, 2997, 1473, 16, 13, 426, 3481, 320, 66552, 45770, 56215, 22717, 811, 505, 81632, 340, 17, 13, 12093, 62537, 64, 320, 14804, 592, 398, 31197, 1534, 426, 3481, 5075, 31754, 54184, 340, 18, 13, 30981, 7099, 198, 19, 13, 5843, 35627, 320, 6720, 31608, 81632, 369, 35081, 39841, 48698, 340, 20, 13, 350, 20, 320, 1199, 4791, 12, 1199, 24078, 63479, 696, 9673, 4211, 617, 17427, 1614, 8838, 10826, 38921, 5178, 304, 5370, 5933, 4221, 8863, 9256, 11, 1778, 439, 1495, 24790, 11, 3488, 36864, 11, 27065, 6492, 11, 323, 5780, 14807, 13], 'total_duration': 7264624750, 'load_duration': 33978584, 'prompt_eval_count': 47, 'prompt_eval_duration': 160275000, 'eval_count': 377, 'eval_duration': 7069473000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_request_to_llm(payload: dict, config: dict):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            config[\"model_endpoint\"],\n",
    "            headers=config[\"headers\"],\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Send the request\n",
    "response = send_request_to_llm(payload=payload, config=llm_config)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sends the prompt to the LLM and handles any errors that might occur during the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the LLM Response\n",
    "\n",
    "Finally, let's process the response and display the relevant information in a user-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed response: Large Language Models (LLMs) have several key features that enable them to process and generate human-like language:\n",
      "\n",
      "1. **Self-Supervised Learning**: LLMs learn from vast amounts of text data without explicit supervision, using techniques like masked language modeling or next sentence prediction.\n",
      "2. **Transformer Architecture**: Most popular LLMs are based on the Transformer architecture, which allows for parallel processing of input sequences and enables efficient computation.\n",
      "3. **Multilayer Perceptron (MLP) Layers**: LLMs typically consist of a series of MLP layers with self-attention mechanisms that weigh inputs according to their relevance.\n",
      "4. **Self-Attention Mechanisms**: These allow the model to focus on specific parts of the input sequence when making predictions, emulating human language processing.\n",
      "5. **Pretraining and Fine-tuning**: LLMs are pre-trained on large datasets and can then be fine-tuned for a specific task or domain.\n",
      "6. **Scalability**: LLMs can process sequences of varying lengths, from short text fragments to long documents.\n",
      "7. **Contextual Understanding**: They capture contextual relationships between words, phrases, and sentences, enabling the model to understand nuances and subtleties in language.\n",
      "8. **Generative Capabilities**: Many LLMs have generative capabilities, allowing them to produce coherent and contextually relevant text.\n",
      "\n",
      "Some popular examples of Large Language Models include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
      "3. XLNet\n",
      "4. Longformer (Long-range Transformers for Automatic Speech Recognition)\n",
      "5. T5 (Text-to-Text Transfer Transformer)\n",
      "\n",
      "These models have achieved state-of-the-art performance in various natural language processing tasks, such as text classification, question answering, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "def process_response(response: dict):\n",
    "    if \"error\" in response:\n",
    "        return f\"Error: {response['error']}\"\n",
    "    return response.get(\"response\", \"No response from the model\")\n",
    "\n",
    "# Process the LLM response\n",
    "processed_response = process_response(response)\n",
    "print(\"Processed response:\", processed_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
