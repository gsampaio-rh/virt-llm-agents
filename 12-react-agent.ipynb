{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a ReAct Agent with Tool Integration\n",
    "\n",
    "In this notebook, you will learn how to create and deploy a ReAct (Reasoning + Acting) agent capable of solving complex tasks by combining logical reasoning with external tool usage. The agent will use a structured loop of **Thought → Action → Observation** to iteratively reason through problems, gather data, and provide solutions.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Implement a ReAct agent that breaks down complex user queries into smaller, manageable steps.\n",
    "- Integrate external tools (e.g., calculators, data retrieval functions) that the agent can use to enhance its problem-solving abilities.\n",
    "- Efficiently handle tool outputs and errors, allowing the agent to adapt its actions based on real-time feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a ReAct Prompting?\n",
    "\n",
    "**ReAct prompting** is a technique that combines reasoning and acting within large language models (LLMs) to solve tasks more effectively. By prompting the model with **task-solving trajectories**, ReAct enables the model to think through a problem step by step while simultaneously taking actions, such as retrieving information or using external tools. This synergy between reasoning and acting allows the model to both plan and execute in a flexible manner.\n",
    "\n",
    "ReAct prompting is particularly effective because it integrates two important processes:\n",
    "- **Reasoning**: The model thinks through the task by generating reasoning traces, helping break down complex queries into manageable steps.\n",
    "- **Acting**: The model performs actions, such as interacting with external tools, APIs, or databases, to gather real-time information that informs its reasoning.\n",
    "\n",
    "For more information, you can read the original paper on ReAct prompting: [ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io/).\n",
    "\n",
    "### How Does ReAct Prompting Work?\n",
    "\n",
    "ReAct prompting follows a **Thought → Action → Observation** loop. Here's how it works:\n",
    "- **Thought**: The model reasons about the task and decides what action to take next.\n",
    "- **Action**: The model executes the chosen action, such as querying an API, calculating a result, or retrieving information.\n",
    "- **Observation**: The model observes the result of the action, updates its internal reasoning, and decides if further actions are needed.\n",
    "\n",
    "This loop continues until the model gathers enough information to provide a complete answer or determines that no further actions are required.\n",
    "\n",
    "### Why Use ReAct Prompting?\n",
    "\n",
    "ReAct prompting allows LLMs to achieve state-of-the-art performance across various tasks by enhancing both reasoning and acting capabilities. It addresses several limitations:\n",
    "- **Misinformation**: In cases where reasoning alone (e.g., chain-of-thought) may lead to errors due to reliance on internal knowledge, ReAct grounding in external actions prevents misinformation.\n",
    "- **Lack of Synthesis**: Acting alone without reasoning can result in incomplete or incoherent solutions. ReAct allows for better synthesis of final answers by combining both reasoning and actions.\n",
    "\n",
    "For example, in the context of our notebook, we'll use ReAct prompting to instruct an agent to use tools like a `basic_calculator` to perform mathematical operations. The agent reasons about when to use the tool, performs the operation, and observes the result to determine if more actions are needed before completing the task.\n",
    "\n",
    "![image.png](images/react-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Before we begin, let's make sure your environment is set up correctly. We'll start by installing the necessary Python packages.\n",
    "\n",
    "### Installing Required Packages\n",
    "\n",
    "To get started, you'll need to install a few Python libraries. Run the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: langgraph in ./.conda/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: langgraph-checkpoint-sqlite in ./.conda/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in ./.conda/lib/python3.11/site-packages (from langchain) (0.2.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.conda/lib/python3.11/site-packages (from langchain) (0.1.110)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in ./.conda/lib/python3.11/site-packages (from langgraph) (1.0.8)\n",
      "Requirement already satisfied: aiosqlite<0.21.0,>=0.20.0 in ./.conda/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.8)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./.conda/lib/python3.11/site-packages (from aiosqlite<0.21.0,>=0.20.0->langgraph-checkpoint-sqlite) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.25.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langgraph langgraph-checkpoint-sqlite requests termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These packages are used for:\n",
    "\n",
    "- **langchain:** A framework for developing applications powered by language models, with support for building agents, managing prompts, and integrating external tools.\n",
    "- **langgraph:** A workflow orchestration library designed to integrate with LangChain, allowing you to create complex AI-driven workflows and reasoning paths.\n",
    "- **langgraph-checkpoint-sqlite:** A tool used for storing checkpoints and managing workflow states using SQLite, ensuring the agent's progress is saved and retrievable.\n",
    "- **requests:** Making HTTP requests to interact with APIs and external services.\n",
    "- **termcolor:** Adding colored text output to the terminal, which is useful for debugging and improving the readability of logs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Function\n",
    "\n",
    "We'll create is a simple function to get the current time. This is important because our agent might need to timestamp certain actions or events. Let's write a function that returns the current date and time in UTC format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current UTC datetime: 2024-09-10 14:09:39.446498 \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def get_current_utc_datetime():\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S.%f UTC\")[:-3]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(\"Current UTC datetime:\", get_current_utc_datetime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring a Simple Model\n",
    "\n",
    "In this section, we configure the machine learning model that our agent will use to process tasks. The `ModelService` class manages the interaction with the model (in this case, \"llama3.1:8b-instruct-fp16\"), allowing the agent to handle tasks such as listing VMs and retrieving details.\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "We initialize the `ModelService` with a specific model configuration, including parameters such as model endpoint, temperature (for controlling randomness), and others. This step enables our agent to perform model-based tasks using the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.model_service import ModelService\n",
    "\n",
    "# Initialize the service with the model configuration\n",
    "ollama_service = ModelService(model=\"llama3.1:8b-instruct-fp16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Tools\n",
    "\n",
    "In this section, we demonstrate how to integrate custom tools into the ReAct agent's workflow. These tools allow the agent to perform specific actions based on the task requirements. We'll start with a basic calculator tool that can perform fundamental arithmetic operations.\n",
    "\n",
    "### Basic Calculator Tool\n",
    "\n",
    "The `basic_calculator` tool performs basic arithmetic operations like addition, subtraction, multiplication, and division. The tool accepts two numbers and an operation as input and returns the result.\n",
    "\n",
    "#### Supported Operations:\n",
    "- `add`: Adds two numbers.\n",
    "- `subtract`: Subtracts the second number from the first.\n",
    "- `multiply`: Multiplies two numbers.\n",
    "- `divide`: Divides the first number by the second (raises an exception for division by zero).\n",
    "- `modulus`: Finds the remainder when the first number is divided by the second.\n",
    "- `power`: Raises the first number to the power of the second.\n",
    "- Comparison operators: `lt` (less than), `le` (less than or equal to), `eq` (equal to), `ne` (not equal to), `ge` (greater than or equal to), `gt` (greater than).\n",
    "\n",
    "The agent will invoke this tool based on the reasoning process and provide structured input in the form of JSON. Let's take a look at the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def basic_calculator(num1, num2, operation):\n",
    "    \"\"\"\n",
    "    Perform a numeric operation on two numbers based on the input string.\n",
    "\n",
    "    Parameters:\n",
    "    'num1' (int): The first number.\n",
    "    'num2' (int): The second number.\n",
    "    'operation' (str): The operation to perform. Supported operations are 'add', 'subtract',\n",
    "                        'multiply', 'divide', 'floor_divide', 'modulus', 'power', 'lt',\n",
    "                        'le', 'eq', 'ne', 'ge', 'gt'.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted result of the operation.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If an error occurs during the operation (e.g., division by zero).\n",
    "    ValueError: If an unsupported operation is requested or input is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the supported operations\n",
    "    operations = {\n",
    "        \"add\": operator.add,\n",
    "        \"subtract\": operator.sub,\n",
    "        \"multiply\": operator.mul,\n",
    "        \"divide\": operator.truediv,\n",
    "        \"floor_divide\": operator.floordiv,\n",
    "        \"modulus\": operator.mod,\n",
    "        \"power\": operator.pow,\n",
    "        \"lt\": operator.lt,\n",
    "        \"le\": operator.le,\n",
    "        \"eq\": operator.eq,\n",
    "        \"ne\": operator.ne,\n",
    "        \"ge\": operator.ge,\n",
    "        \"gt\": operator.gt,\n",
    "    }\n",
    "\n",
    "    # Check if the operation is supported\n",
    "    if operation in operations:\n",
    "        try:\n",
    "            # Perform the operation\n",
    "            result = operations[operation](num1, num2)\n",
    "            result_formatted = (\n",
    "                f\"The answer is: {result}.\\nCalculated with basic_calculator.\"\n",
    "            )\n",
    "            return result_formatted\n",
    "        except Exception as e:\n",
    "            return str(e), \"\\n\\nError during operation execution.\"\n",
    "    else:\n",
    "        return \"\\n\\nUnsupported operation. Please provide a valid operation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [basic_calculator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a ReAct Agent\n",
    "\n",
    "In this section, we will build a ReAct agent that autonomously reasons through tasks and selects the appropriate tools to solve problems. The agent will follow a structured process to deliver actionable results.\n",
    "\n",
    "### Architecture Explanation\n",
    "\n",
    "- **Agent Components**:\n",
    "  - **Memory**: The agent's memory module stores past interactions and relevant data to inform future decisions.\n",
    "  - **Planning**: This module breaks down complex tasks into manageable steps, helping the agent decide the sequence of actions.\n",
    "  - **Tools**: The tools module enables the agent to interact with external systems, such as performing calculations, retrieving data, or running APIs.\n",
    "\n",
    "- **System Prompt & User Prompt**: \n",
    "  - The **System Prompt** provides context and instructions for how the agent should operate within its environment, including the tools available.\n",
    "  - The **User Prompt** is the input provided by the user, specifying the task or query the agent needs to handle.\n",
    "\n",
    "- **LLM Interaction**: \n",
    "  - The Large Language Model (LLM) is at the core of the agent’s decision-making. It processes both the system and user prompts, and its outputs guide the agent’s actions. The LLM can reason about tasks, plan, and adapt based on external tool outputs.\n",
    "  \n",
    "- **External Environments**:\n",
    "  - The **Local Environment** consists of tools and systems the agent interacts with locally on its workstation.\n",
    "  - The **External Environment** is any external system or API that the agent can call for additional information or actions.\n",
    "\n",
    "This architecture allows the ReAct agent to efficiently break down complex tasks, take action, and observe the results in a structured, repeatable process.\n",
    "\n",
    "![React Agent Architecture](images/react_agent_architecture_modules.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "The system prompt provides the instructions that guide the agent in reasoning through tasks, using tools, and generating structured responses. It establishes the context, including the environment (e.g., ipython) and knowledge cut-off date (December 2023), ensuring the agent understands the limits of its information.\n",
    "\n",
    "The agent is tasked with using tools to solve problems and must decide which tool to use and in what sequence. Each interaction follows a structured JSON format, ensuring clarity in both tool inputs and outputs.\n",
    "\n",
    "The agent operates in a cycle of **thought → action → observation**:\n",
    "- **Thought**: The agent thinks about the task and determines the next action.\n",
    "- **Action**: The agent selects and uses the appropriate tool.\n",
    "- **Observation**: The agent analyzes the tool's result and decides the next step.\n",
    "\n",
    "This process repeats until the agent reaches a sufficient conclusion to answer the user’s query. If the tool provides a clear result, the agent will stop further actions and present the final answer. If the task cannot be completed, the agent will explain the limitation and provide suggestions.\n",
    "\n",
    "The system prompt ensures that the agent behaves logically, utilizes tools efficiently, and delivers structured and coherent responses.\n",
    "\n",
    "For more details on LLAMA 3.1, refer to the [LLAMA 3.1 Model Card](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n",
    "\n",
    "![React System Prompt](images/react_system_prompt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYS_REACT_PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: {tools_name} \n",
    "Knowledge Cutoff Date: December 2023  \n",
    "Current Date: {datetime}\n",
    "\n",
    "You are an intelligent assistant designed to handle various tasks, including answering questions, providing summaries, and performing detailed analyses. All outputs must strictly be in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools\n",
    "You have access to a variety of tools to assist in completing tasks. You are responsible for determining the appropriate sequence of tool usage to break down complex tasks into subtasks when necessary.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format:\n",
    "To complete the task, please use the following format:\n",
    "\n",
    "{{\n",
    "  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ # Provide valid JSON input for the action, ensuring it matches the tool’s expected format and data types.\n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "After performing an action, the tool will provide a response in the following format:\n",
    "\n",
    "{{\n",
    "  \"observation\": \"The result of the tool invocation\",\n",
    "}}\n",
    "\n",
    "You should keep repeating the format (thought → action → observation) until you have the answer to the original question. \n",
    "\n",
    "If the tool result is successful and the task is complete:\n",
    "\n",
    "{{\n",
    "  \"answer\": \"I have the answer: {{tool_result}}.\"\n",
    "}}\n",
    "\n",
    "\n",
    "Or, if you cannot answer:\n",
    "\n",
    "{{\n",
    "  \"answer\": \"Sorry, I cannot answer your query.\"\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "### Remember:\n",
    "- **If a tool provides a complete and clear answer, do not continue invoking further tools.**\n",
    "- Use the tools effectively and ensure inputs match the required format exactly as described in the task.\n",
    "- Maintain the JSON format and ensure all fields are filled out correctly.\n",
    "- Do not include additional metadata such as `title`, `description`, or `type` in the `tool_input`.\n",
    "\n",
    "<|eot_id|>\n",
    "{user_prompt}\n",
    "{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Agent Class\n",
    "\n",
    "In this section, we define the `ReActAgent` class, responsible for managing an agent that processes tasks based on user input. The class handles interactions with the language model, executes tools, and follows an action-observation loop until a final answer is generated.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Model Invocation (`invoke_model`)**:\n",
    "   - Prepares the input, sends it to the language model, and processes the response. This is the main interface for interacting with the model using system and user prompts.\n",
    "\n",
    "2. **ReAct Loop (`react`)**:\n",
    "   - Implements the **Thought → Action → Observation** cycle:\n",
    "     - The agent processes the user’s request, interacts with the model, and checks for an action or final answer.\n",
    "     - If an action is required, the agent executes the tool, observes the result, and continues the loop until a final answer is produced.\n",
    "\n",
    "3. **Tool Execution (`execute_tool`)**:\n",
    "   - Simulates tool execution based on the model’s action request. In practice, this could involve calling real-world tools or APIs. The result is returned to the agent as an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "from termcolor import colored\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from services.model_service import ModelService\n",
    "from utils.general.tools import get_tools_name, get_tools_description\n",
    "\n",
    "\n",
    "class ReactAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: Dict[str, Any],\n",
    "        role: str,\n",
    "        tools: list,\n",
    "        ollama_service: ModelService,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Agent with a state, role, and model configuration.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.role = role\n",
    "        self.tools = tools\n",
    "        self.ollama_service = ollama_service\n",
    "\n",
    "        # Initialize lists to accumulate responses if they don't exist in the state\n",
    "        if \"messages\" not in self.state:\n",
    "            self.state[\"messages\"] = []\n",
    "\n",
    "    def invoke_model(self, sys_prompt: str, user_prompt: str):\n",
    "        \"\"\"\n",
    "        Prepare the payload, send the request to the model, and process the response.\n",
    "        \"\"\"\n",
    "        # Prepare the payload\n",
    "        payload = self.ollama_service.prepare_payload(\n",
    "            user_prompt,\n",
    "            sys_prompt,\n",
    "        )\n",
    "\n",
    "        # Invoke the model and get the response\n",
    "        response_json = self.ollama_service.request_model_generate(\n",
    "            payload,\n",
    "        )\n",
    "\n",
    "        # Process the model's response\n",
    "        response_content = self.ollama_service.process_model_response(response_json)\n",
    "\n",
    "        # Return the processed response\n",
    "        return response_content\n",
    "\n",
    "    def write_react_prompt(\n",
    "        self,\n",
    "        user_prompt: str = \"\",\n",
    "        agent_scratchpad: str = \"\",\n",
    "    ) -> str:\n",
    "        return DEFAULT_SYS_REACT_PROMPT.format(\n",
    "            user_prompt=user_prompt,\n",
    "            agent_scratchpad=agent_scratchpad,\n",
    "            tools_name=get_tools_name(self.tools),\n",
    "            tools_description=get_tools_description(self.tools),\n",
    "            datetime=get_current_utc_datetime(),\n",
    "        )\n",
    "\n",
    "    # Function to format the scratchpad into a properly indented string\n",
    "    def format_scratchpad(self, scratchpad):\n",
    "        formatted_output = \"\"\n",
    "        for entry in scratchpad:\n",
    "            formatted_output += entry.strip() + \"\\n\"\n",
    "        return formatted_output\n",
    "\n",
    "    def react(self, user_request: str) -> dict:\n",
    "        \"\"\"\n",
    "        Execute the task based on the user's request by following the thought → action → observation loop.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state[\"messages\"].append(HumanMessage(content=user_request))\n",
    "\n",
    "        answer = None\n",
    "\n",
    "        # Start with the user's request as the first input\n",
    "        user_prompt = (\n",
    "            f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\"\"\"\n",
    "        )\n",
    "\n",
    "        sys_prompt = self.write_react_prompt(user_prompt=user_prompt)\n",
    "        # user_prompt = user_request\n",
    "        tool_response = None\n",
    "        action = None\n",
    "        action_input = None\n",
    "        scratchpad = []\n",
    "\n",
    "        print(colored(user_prompt, \"green\"))\n",
    "\n",
    "        # Loop until a final answer is generated\n",
    "        while answer is None:\n",
    "            # Invoke the model with the system prompt and current user input\n",
    "\n",
    "            response = self.invoke_model(sys_prompt=sys_prompt, user_prompt=user_prompt)\n",
    "\n",
    "            try:\n",
    "                # Parse the response assuming it's in JSON format\n",
    "                response_dict = json.loads(\n",
    "                    response\n",
    "                )  # Assuming response is a JSON object\n",
    "\n",
    "                assistant_message = f\"\"\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\"\"\n",
    "\n",
    "                print(colored(assistant_message, \"cyan\"))\n",
    "                self.state[\"messages\"].append(AIMessage(content=response))\n",
    "\n",
    "                scratchpad.append(assistant_message)\n",
    "\n",
    "                formatted_scratchpad = self.format_scratchpad(scratchpad)\n",
    "                sys_prompt = self.write_react_prompt(\n",
    "                    user_prompt=user_prompt, agent_scratchpad=formatted_scratchpad\n",
    "                )\n",
    "\n",
    "                action = response_dict.get(\"action\", None)\n",
    "                action_input = response_dict.get(\"action_input\", None)\n",
    "\n",
    "                # If there is an action, execute the corresponding tool\n",
    "                if action:\n",
    "                    status, tool_response = self.execute_tool(action, action_input)\n",
    "\n",
    "                    # Formulate the observation to feed back into the model\n",
    "                    tool_response_dict = {\n",
    "                        \"observation\": tool_response,\n",
    "                    }\n",
    "\n",
    "                    tool_response_json = json.dumps(tool_response_dict, indent=4)\n",
    "\n",
    "                    result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{tool_response_json}<|eot_id|>\"\"\"\n",
    "\n",
    "                    print(colored(result_message, \"yellow\"))\n",
    "\n",
    "                    user_prompt = tool_response_json\n",
    "                    # Append the tool response to the state\n",
    "                    self.state[\"messages\"].append(\n",
    "                        SystemMessage(content=tool_response)\n",
    "                    )\n",
    "\n",
    "                # Check if the model has given an answer\n",
    "                if \"answer\" in response_dict:\n",
    "                    answer = response_dict[\"answer\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                system_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{str(e)}<|eot_id|>\"\"\"\n",
    "                scratchpad.append(system_message)\n",
    "                formatted_scratchpad = self.format_scratchpad(scratchpad)\n",
    "                sys_prompt = self.write_react_prompt(\n",
    "                    user_prompt=user_prompt, agent_scratchpad=formatted_scratchpad\n",
    "                )\n",
    "\n",
    "        # Return the final answer\n",
    "        return self.state\n",
    "\n",
    "    def execute_tool(self, action: str, action_input: dict):\n",
    "        \"\"\"\n",
    "        Simulate the tool execution based on the action and action_input.\n",
    "        In a real-world scenario, this would call the appropriate tool.\n",
    "        \"\"\"\n",
    "        # Simulate some tool actions (this would be replaced by actual tool logic)\n",
    "        tool_message = f\"\"\"<|python_tag|>{action}.call({action_input})\\n<|eom_id|>\"\"\"\n",
    "        print(\n",
    "            colored(\n",
    "                tool_message,\n",
    "                \"magenta\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for tool in self.tools:\n",
    "            if tool.name == action:\n",
    "                try:\n",
    "                    result = tool.invoke(action_input)\n",
    "                    result_message = f\"\"\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{result}<|eot_id|>\"\"\"\n",
    "                    print(colored(result_message, \"magenta\"))\n",
    "                    return True, result\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error executing tool {action}: {str(e)}\"\n",
    "        else:\n",
    "            return f\"Tool {action} not found or unsupported operation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the Agent\n",
    "\n",
    "In this section, we will execute the `ReActAgent` to demonstrate how it processes user input, interacts with tools, and follows the **Thought → Action → Observation** loop to produce a final answer.\n",
    "\n",
    "### Steps to Run the Agent:\n",
    "\n",
    "1. **Initialize the Agent**: \n",
    "   - We start by setting up the initial state, defining the agent's role, specifying the available tools, and connecting the agent to a language model service.\n",
    "   \n",
    "2. **User Request**: \n",
    "   - A user query is provided as input, such as \"What is the sum of 15 and 27?\" The agent receives this request and starts reasoning through it.\n",
    "\n",
    "3. **Processing the Request**:\n",
    "   - The agent constructs a system prompt with the user’s input and interacts with the language model in a continuous loop, checking for actions or final answers.\n",
    "   - If a tool is needed (e.g., a calculator), the agent will invoke the appropriate tool, observe the result, and feed the output back into its reasoning process.\n",
    "\n",
    "4. **Final Output**:\n",
    "   - The agent continues looping through the **Thought → Action → Observation** cycle until a final answer is generated or the task cannot be completed. Once complete, the result is stored in the agent’s state, and the final answer is returned.\n",
    "\n",
    "By the end of this process, the agent will have used reasoning and external tools to autonomously solve the problem presented by the user.\n",
    "\n",
    "![React Flow](images/react_flow.png)\n",
    "\n",
    "### Example\n",
    "\n",
    "In the next cell, we provide an example where the agent solves a mathematical problem by using a tool and returning the result.\n",
    "\n",
    "Running this example will give you insight into how the ReAct agent works in practice, allowing it to break down complex tasks, interact with tools, and generate structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 10+10?<|eot_id|>\u001b[0m\n",
      "\u001b[36m<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"thought\": \"The problem requires a basic arithmetic operation, so I will use the 'basic_calculator' tool.\",\n",
      "    \"action\": \"basic_calculator\",\n",
      "    \"action_input\": {\n",
      "        \"num1\": 10,\n",
      "        \"num2\": 10,\n",
      "        \"operation\": \"add\"\n",
      "    }\n",
      "}<|eot_id|>\u001b[0m\n",
      "\u001b[35m<|python_tag|>basic_calculator.call({'num1': 10, 'num2': 10, 'operation': 'add'})\n",
      "<|eom_id|>\u001b[0m\n",
      "\u001b[35m<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "The answer is: 20.\n",
      "Calculated with basic_calculator.<|eot_id|>\u001b[0m\n",
      "\u001b[33m<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"observation\": \"The answer is: 20.\\nCalculated with basic_calculator.\"\n",
      "}<|eot_id|>\u001b[0m\n",
      "\u001b[36m<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "    \"answer\": \"I have the answer: 20.\"\n",
      "}<|eot_id|>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define the initial state, role, tools, and the model service instance\n",
    "initial_state = {\n",
    "    \"messages\": [],  # To store agent's responses\n",
    "}\n",
    "\n",
    "react_agent = ReactAgent(\n",
    "    state=initial_state,\n",
    "    role=\"REACT_AGENT\",\n",
    "    tools=tools,\n",
    "    ollama_service=ollama_service,\n",
    ")\n",
    "\n",
    "user_input = \"What is 10+10?\"\n",
    "\n",
    "final_state = react_agent.react(user_request=user_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages from the Agent's State:\n",
      "\u001b[32m================================ Human Message =================================\n",
      "\n",
      "What is 10+10?\u001b[0m\n",
      "\u001b[36m================================== Ai Message ==================================\n",
      "\n",
      "{\n",
      "    \"thought\": \"The problem requires a basic arithmetic operation, so I will use the 'basic_calculator' tool.\",\n",
      "    \"action\": \"basic_calculator\",\n",
      "    \"action_input\": {\n",
      "        \"num1\": 10,\n",
      "        \"num2\": 10,\n",
      "        \"operation\": \"add\"\n",
      "    }\n",
      "}\u001b[0m\n",
      "\u001b[33m================================ System Message ================================\n",
      "\n",
      "The answer is: 20.\n",
      "Calculated with basic_calculator.\u001b[0m\n",
      "\u001b[36m================================== Ai Message ==================================\n",
      "\n",
      "{\n",
      "    \"answer\": \"I have the answer: 20.\"\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Messages from the Agent's State:\")\n",
    "\n",
    "for message in final_state[\"messages\"]:\n",
    "    pretty_message = message.pretty_repr()\n",
    "    if isinstance(message, AIMessage):\n",
    "        print(colored(f\"{pretty_message}\", \"cyan\"))\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        print(colored(f\"{pretty_message}\", \"yellow\"))\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        print(colored(f\"{pretty_message}\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we successfully built a ReAct agent capable of autonomously solving tasks by reasoning through problems and interacting with external tools. We explored key concepts such as the **Thought → Action → Observation** loop, where the agent breaks down complex tasks into manageable steps, executes actions via tools, and refines its approach based on real-time feedback.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **ReAct Prompting**: The agent uses a combination of reasoning and action to solve tasks, invoking tools dynamically based on the problem at hand.\n",
    "- **Agent Architecture**: We learned how different components such as memory, planning, and tools integrate with the LLM to enable the agent’s decision-making process.\n",
    "- **Tool Integration**: We demonstrated how the agent interacts with external systems and tools to enhance its capabilities, making it more effective at solving complex, multi-step tasks.\n",
    "- **System Prompts and User Prompts**: Properly crafting prompts ensures the agent operates within its environment and understands the scope of its tasks.\n",
    "\n",
    "By the end of this notebook, you should have a solid understanding of how ReAct agents work and how they can be employed to tackle a wide range of tasks. You can now extend this framework to include more sophisticated tools or refine the agent’s reasoning abilities to suit more advanced applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
