{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building a Simple Agent in Python\n",
    "\n",
    "Welcome to this tutorial on building a simple agent in Python! In this notebook, we will focus on creating a basic agent that can communicate with a **Large Language Model (LLM)** to perform tasks autonomously. We will build upon the concepts you learned in the previous notebook, such as prompts and LLM interaction, and now put those into practice to create a functional agent.\n",
    "\n",
    "By the end of this tutorial, you will know how to configure a simple agent, interact with an LLM, and process the responses it provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Agent?\n",
    "\n",
    "An **Agent** is an autonomous system that leverages **Large Language Models (LLMs)** to perform tasks by understanding, reasoning, planning, and executing actions with minimal human intervention. AI agents are designed to break down complex problems into manageable steps, utilizing tools, accessing memory, and adapting their behavior based on the provided context.\n",
    "\n",
    "At its core, an agent is structured to:\n",
    "\n",
    "1. **Receive a Task**: The agent takes input from the user, such as a question or command.\n",
    "2. **Plan a Solution**: The agent decomposes the problem, chooses appropriate tools, and reasons through possible solutions.\n",
    "3. **Execute the Plan**: It performs actions, such as retrieving information, using tools, or generating responses based on the devised plan.\n",
    "4. **Deliver Results**: Finally, it presents the solution or output in a structured, actionable format.\n",
    "\n",
    "![agenticvnonagentic](images/agentic-vs-non-agentic.png)\n",
    "\n",
    "For more in-depth analysis on LLM-based autonomous agents, refer to this survey: [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432)\n",
    "\n",
    "### Key Components of an AI Agent:\n",
    "\n",
    "- **Profiling Module (Agent Core)**: This is the agent's decision-making hub. It defines the role and goals of the agent (e.g., financial analyst, teacher), selects appropriate tools, and coordinates task execution. The agent's \"profile\" helps determine its behavior and interaction style based on its role.\n",
    "  \n",
    "- **Memory Module**: The agent uses memory to track past interactions and experiences. Short-term memory stores context-relevant information (e.g., current session details), while long-term memory retains important information over time, which the agent can refer back to when needed.\n",
    "\n",
    "- **Tools Module**: External resources (e.g., APIs, databases) that the agent can call upon to complete tasks, like retrieving real-time data, performing calculations, or interacting with other systems.\n",
    "\n",
    "- **Planning Module**: This module allows the agent to break down complex tasks into smaller, manageable sub-tasks. By planning step-by-step, the agent can tackle intricate queries and tasks with greater efficiency and precision.\n",
    "\n",
    "![agentmodules](images/agent_modules_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Consider an agent designed to assist with financial analysis. When asked, \"What are the three key takeaways from the Q2 earnings call for FY 2023?\", the agent processes the query in a few steps:\n",
    "1. It first identifies the relevant sections of the earnings call.\n",
    "2. Next, it analyzes the key points related to business or technology developments.\n",
    "3. Lastly, the agent summarizes and presents the findings clearly to the user.\n",
    "\n",
    "This example highlights how AI agents harness the reasoning capabilities of an LLM, combined with external tools and memory, to generate well-structured and detailed responses to complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "Before we can communicate with the LLM, let’s install any required libraries and ensure our environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests jsonschema tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These packages are used for:\n",
    "\n",
    "- **requests:** Making HTTP requests to interact with models.\n",
    "- **jsonschema:** Validating the structure of the agent's output.\n",
    "- **tenacity:** Handling retries in case of errors when communicating with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Function\n",
    "\n",
    "We'll create is a simple function to get the current time. This is important because our agent might need to timestamp certain actions or events. Let's write a function that returns the current date and time in UTC format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def get_current_utc_datetime():\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    return now_utc.strftime(\"%Y-%m-%d %H:%M:%S.%f UTC\")[:-3]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(\"Current UTC datetime:\", get_current_utc_datetime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring a Simple Model\n",
    "\n",
    "In this section, we configure the machine learning model that our agent will use to process tasks. The `ModelService` class manages the interaction with the model (in this case, \"llama3.1:8b-instruct-fp16\"), allowing the agent to handle tasks such as listing VMs and retrieving details.\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "We initialize the `ModelService` with a specific model configuration, including parameters such as model endpoint, temperature (for controlling randomness), and others. This step enables our agent to perform model-based tasks using the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.model_service import ModelService\n",
    "\n",
    "# Initialize the service with the model configuration\n",
    "ollama_service = ModelService(model=\"llama3.1:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompt for the Simple Agent\n",
    "\n",
    "In this cell, we define the **System Prompt** for our agent. The system prompt helps set the behavior of the agent when interacting with the **Large Language Model (LLM)**. This particular prompt instructs the LLM to act as a helpful assistant, focused on answering questions clearly and concisely.\n",
    "\n",
    "### Key Aspects of the System Prompt:\n",
    "1. **Assistant Role:** The agent is instructed to provide useful, clear, and concise responses to user queries.\n",
    "2. **Guidelines:**\n",
    "   - **Clear Responses:** The agent is expected to give direct and simple answers to the user's questions.\n",
    "   - **Avoid Unnecessary Details:** It avoids overloading the user with too much information—responses should be brief and relevant.\n",
    "   - **Polite Tone:** The agent will always maintain a friendly and helpful tone in its responses.\n",
    "   - **Answer Format:** Responses are expected to be returned in simple text, with no special formatting unless specifically requested by the user.\n",
    "   \n",
    "### Example Interaction:\n",
    "This system prompt also includes an example to illustrate the expected behavior of the agent:\n",
    "- If the user asks, \"What is Python used for?\", the agent will respond with something like:\n",
    "  - \"Python is a versatile programming language used for web development, data analysis, automation, and more.\"\n",
    "\n",
    "This **System Prompt** ensures that the agent remains consistent in how it responds, providing useful and easy-to-understand answers in all cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutoff Knowledge Date: December 2023\n",
    "Current Date: {datetime}\n",
    "\n",
    "You are a helpful assistant. Your job is to answer questions clearly and concisely. Make sure your responses are easy to understand and provide useful information.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Interaction:\n",
    "\n",
    "User: \"What is Python used for?\"\n",
    "\n",
    "Assistant: \"Python is a versatile programming language used for web development, data analysis, automation, and more.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points:\n",
    "- **Be Direct:** Answer only what is asked, without extra information.\n",
    "- **Be Polite:** Maintain a friendly and helpful tone.\n",
    "<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Agent Class for Interacting with the LLM\n",
    "\n",
    "In this cell, we define the `Agent` class, which serves as the core component responsible for interacting with the **Large Language Model (LLM)**. This class is designed to handle the interaction between user input and the LLM, process the model's responses, and maintain an internal state.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Initialization (`__init__`):**\n",
    "   - The `Agent` class is initialized with:\n",
    "     - `state`: A dictionary to track the agent's current state.\n",
    "     - `role`: The defined role of the agent (e.g., \"assistant\").\n",
    "     - `ollama_service`: A service interface that handles communication with the LLM (in this case, represented by `ModelService`).\n",
    "\n",
    "2. **State Management (`update_state`):**\n",
    "   - The `update_state` method allows the agent to update its internal state based on key-value pairs. This helps the agent track progress or store important data.\n",
    "   - If the agent tries to update a state key that doesn't exist, it will warn the user.\n",
    "\n",
    "3. **Model Interaction (`invoke_model`):**\n",
    "   - This is the core method that interacts with the LLM. It prepares the input payload by combining the **system prompt** and **user prompt**.\n",
    "   - The payload is then sent to the model service (`ollama_service`), which communicates with the LLM, retrieves the response, and processes it.\n",
    "   - The final processed response from the model is returned for further use.\n",
    "\n",
    "4. **Main Task Execution (`work`):**\n",
    "   - The `work` method is the primary interface for executing tasks based on user input.\n",
    "   - It formats the **system prompt** (using a predefined prompt template) and combines it with the **user prompt** (the user's specific question or request).\n",
    "   - The `work` method then invokes the model and returns the final response from the LLM.\n",
    "   - This method can be easily extended for more complex tasks.\n",
    "\n",
    "### Example Workflow:\n",
    "- The agent receives a **user request**.\n",
    "- It prepares the system and user prompts and sends them to the model.\n",
    "- The LLM processes the input and returns a response.\n",
    "- The agent processes the LLM’s response and returns the final output.\n",
    "\n",
    "This structure makes it easy to build on top of the agent, allowing you to handle more complex interactions, manage state efficiently, and extend the agent’s behavior over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state: Dict[str, Any], role: str, ollama_service: ModelService):\n",
    "        \"\"\"\n",
    "        Initialize the Agent with a state, role, and model configuration.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.role = role\n",
    "        self.ollama_service = ollama_service\n",
    "\n",
    "    def update_state(self, key: str, value: Any):\n",
    "        \"\"\"\n",
    "        Update the state of the agent. Warn if the key doesn't exist.\n",
    "        \"\"\"\n",
    "        if key in self.state:\n",
    "            self.state[key] = value\n",
    "        else:\n",
    "            print(f\"Warning: Attempting to update a non-existing state key '{key}'.\")\n",
    "\n",
    "    def invoke_model(self, sys_prompt: str, user_prompt: str):\n",
    "        \"\"\"\n",
    "        Prepare the payload, send the request to the model, and process the response.\n",
    "        \"\"\"\n",
    "        # Prepare the payload\n",
    "        payload = self.ollama_service.prepare_payload(\n",
    "            user_prompt,\n",
    "            sys_prompt,\n",
    "        )\n",
    "\n",
    "        # Invoke the model and get the response\n",
    "        response_json = self.ollama_service.request_model_generate(\n",
    "            payload,\n",
    "        )\n",
    "\n",
    "        # Process the model's response\n",
    "        response_content = self.ollama_service.process_model_response(response_json)\n",
    "\n",
    "        # Return the processed response\n",
    "        return response_content\n",
    "\n",
    "    def work(\n",
    "        self,\n",
    "        user_request: str,\n",
    "        sys_prompt: str = SYS_PROMPT,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Execute a simple task based on the user's request.\n",
    "        \"\"\"\n",
    "        # Define a simple system prompt\n",
    "        formatted_sys_prompt = sys_prompt.format(datetime=get_current_utc_datetime())\n",
    "        user_prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_request}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "        # Invoke the model with the user's request\n",
    "        response = self.invoke_model(\n",
    "            sys_prompt=formatted_sys_prompt, user_prompt=user_prompt\n",
    "        )\n",
    "\n",
    "        # Return the processed response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent\n",
    "\n",
    "Finally, let's demonstrate how to run the agent with a simple example. We'll use the agent class we've just implemented to process a task based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with an empty state and a role\n",
    "agent_state = {\"response\": \"\"}\n",
    "agent_role = \"Helper Agent\"\n",
    "agent = Agent(state=agent_state, role=agent_role, ollama_service=ollama_service)\n",
    "\n",
    "# Execute a task\n",
    "user_input = \"Tell me everything you know about Tesla.\"\n",
    "response = agent.work(user_request=user_input)\n",
    "print(\"Agent's response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we’ve built a **simple AI-powered agent** that interacts with a **Large Language Model (LLM)** to answer user queries. Let’s recap what we've accomplished:\n",
    "\n",
    "- **Defined the agent's behavior** through a **System Prompt**, instructing the LLM on how to respond to questions.\n",
    "- **Created the `Agent` class** to manage interactions between the user and the LLM.\n",
    "- **Implemented state management** to allow the agent to track progress or other necessary information.\n",
    "- **Configured a model service** to handle communication between the agent and the LLM, enabling it to retrieve and process responses.\n",
    "- **Executed the agent** with a user query, receiving a helpful response from the LLM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
